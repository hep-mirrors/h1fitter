\label{sec:man}
%%%%%%%%%%%
\subsection{Steering files}
    The software behaviour is controlled by three files with steering commands.
    These files have predefined names:
    \begin{itemize}
      \item {\tt steering.txt}  --   controls main "stable" (un-modified during 
                         minimisation) parameters. The file also contains
                         names of data files to be fitted to, definition 
                         of kinematic cuts                              
      \item {\tt minuit.in.txt}
                   --  controls minimisation parameters and minimisation 
                         strategy. Standard Minuit commands can be provided
                         in this file
      \item {\tt ewparam.txt}    --  controls electroweak parameters such
         as W and Z boson masses and CKM matrix parameters.
    \end{itemize}
%%%%%%%%%%%

Different options are activated via steering flags in the main steering file.
%and the default steering file is displayed in figure  \ref{fig:steering}.


 
   The format of the steering file follows standard "namelist" conventions.
 Comments start with exclamation mark (similarly used for data file format).
The following namelist blocks are encountered:
\begin{itemize}
\item  {\tt InFiles}: Namelist to control input data
\item  {\tt InCorr}: Namelist to control statistical correlation files
\item  {\tt Scales} (Optional): Namelist to modify renormalisation/factorisation scale
\item  {\tt HeraFitter}: Main steering cards. Further details can be found in the appendix \ref{sec:herafitter}. 
\item  {\tt ExtraMinimisationParameters}:  Namelist to add extra to minuit parameters.
\item  {\tt Output}: Namelist that outputs steering cards 
\item  {\tt Cuts}: Namelist for process dependent cuts
\item  {\tt MCErrors} (Optional):Namelist for MC errors steering cards
\item  {\tt Cheb} (Optional): Chebyshev study namelist
\item  {\tt Poly} (Optional): pure polynomial parameterisation for valence quarks
\item  {\tt HQScale} (Optional): choose the factorisation scale for HQs
\item  {\tt lhapdf} (Optional):LHAPDF sttering card
\item  {\tt reweighting} (Optional): reweighting steering cards
\end{itemize}

These namelist blocks are described in greater details in the User's example \ref{section:example}. 

The specific input files are stored in the {\tt $input\_steerings$} directory and 
it contains the following ready to use inputs (with corresponding minuit files):

\begin{itemize}
\item  {\tt steering.txt.ALLdata}: all data files 
\item  {\tt steering.txt.DIFFRACTION}: diffraction specific settings 
\item  {\tt steering.txt.kt-factorisation}: kt factorisation specific settings
\item  {\tt steering.txt.dipole}: dipole model specific settings  
\end{itemize}


%\subsubsection{Options for DIS fits}
%%%%
%\subsubsection{Options for Jets}
%%%%
%\subsubsection{Options for Diffractive fits}
%%%%
%\subsubsection{Options for DY fits}
%%%%%%%%%%%
\subsection{Data file format}
\label{sec:dataformat}
   Experimental data are provided by the standard {\tt ASCII} text files. The files
   contain a "header" which describes the data format and the "data" in terms
   of a 2-dimensional table. Each line of the data table corresponds to a
   data point, the meaning of the columns is specified in the file header.

   For example, a header for HERA-I combined H1-ZEUS data for e+p neutral 
   current scattering cross section is given in the file

\begin{verbatim}
       datafiles/H1ZEUS_NC_e-p_HERA1.0.dat
\end{verbatim}

   The format of the file follows standard "namelist" conventions. Comments 
   start with exclamation mark.  Pre-defined variables are:
\begin{itemize}
     \item{\tt Name}        --- (string) provides a name of the data set
    \item{\tt  Reaction}    --- (string) reaction type of the data set. Reaction type is used 
                      to trigger corresponding theory calculation. The following 
                      reaction types  are currently supported by the HERAFitter:
                      \begin{itemize}
                        \item {\tt 'NC e+-p'}  -- double differential NC ep scattering
                                      (ZMVFS and RT-VFS schemes) 
                        \item {\tt 'CC e+-p'}  -- double differential CC ep scattering
                                      (ZMVFS scheme)
                        \item {\tt 'CC pp'}    -- single differential $d \sigma_{W^{\pm}}/d eta_{\ell^{\pm}}$
                                      production and W asymmetry at $pp$ and $p\bar{p}$ 
                                      colliders (LO+kfactors and APPLGRID interface)
                        \item {\tt 'NC pp'}    -- single differential $d \sigma_Z / d y_Z$ at $pp$ and
                                      $p\bar{p}$ colliders
                                      ({\tt LO} with k-factors and {\tt APPLGRID} interface)

                        \item 'pp jets APPLGRID' -- $pp\to$ inclusive jet production, using
                                     {\tt APPLGRID}

                        \item 'FastNLO jets' -- jet cross sections using {\tt FastNLO} interface.
                                     All $ep$, $pp$ and $p\bar{p}$ colliders are supported.

                        \item 'FastNLO ep jets normalised' -- jet cross sections in the $ep$ collisions 
                                     using {\tt FastNLO} interface and normalised to the inclusive DIS cross sections.

                      \end{itemize}                       
      \item {\tt NData}       --- (integer) specifies number of data points in the file. 
                     This corresponds to the number of table rows which 
                     follow after the header.
      \item {\tt NColumn}     --- (integer) number of columns in the data table.
      \item {\tt ColumnType}  --- (array of strings)
                      Defines layout of the data table. The following column types
                      are pre-defined: 'Bin', 'Sigma', 'Error' and 'Dummy'
                      The keywords are case sensitive. 'Bin' correspond to an
                      abstract bin definition, 'Sigma' corresponds to the data
                      measurement, 'Error' - to various type of uncertainties and
                      'Dummy' indicates that the column should be ignored.
      \item {\tt ColumnName}  --- (array of strings)
                      Defines names of the columns. The meaning of the name depends
                      on the ColumnType. For ColumnType 'Bin', ColumnName gives a
                      name of the abstract bin. The abstract bins can contain
                      any variable names, but some of them must be present for 
                      correct cross section calculation. For example, 'x', 'Q2' and
                      'y' are required for DIS NC cross-section calculation.
 
                      For ColumnType 'Sigma', ColumnName provides a label for 
                      the observable, which can be any string.
 
                      For ColumnType 'Error', the following names have special meaning:
                      \begin{itemize}
                       \item 'stat'  -- specifies column with statistical uncertainties, request Poisson re-scaling;
                       \item 'stat const'  -- specifies column with statistical uncertainties, request no re-scaling of the errors;
                       \item 'uncor' -- specifies column with uncorrelated uncertainties. Any name containing keyword ``uncor'' is treated as an uncorrelated
  error souce, e.g. ``h1 uncor'';  
                       \item 'uncor const' -- specifies column with uncorrelated uncertainties, request no re-scaling of the errors;  
                       \item 'total' -- specifies column with total uncertainties. 
                                  Total uncertainties are not used in the fit,
                                  however there is an additional check is performed
                                  if 'total' column is specified: sum in quadrature
                                  of statistical, uncorrelated and correlated 
                                  systematic uncertainties is compared to the total
                                  and a warning is issued if they differ significantly.
                       \item'ignore' - specifies column to be ignored (for special studies).
                       \item Other names specifies columns of correlated systematic 
                      uncertainty. For a given data file, each column of the correlated
                      uncertainty must have unique name. To specify correlation across
                      data files, same name must be used for different files.  
                      \end{itemize}
      \item {\tt SystScales}  --- (array of float)
                      For special studies, systematic uncertainties can be scaled
                      The numbering of uncertainties starts from the first column
                      with the ColumnType 'Error'. For example, setting 
\begin{verbatim}
                  SystScale(1) = 2.  
\end{verbatim}
                      in {\tt datafiles/H1ZEUS\_NC\_e-p\_HERA1.0.dat} would scale stat. 
                      uncertainty by factor of two.                       
      \item {\tt Percent}     --- (array of bool) For each uncertainty specify if it is given in 
                      absolute ("false") or in percent ("true").  The numbering of 
                      uncertainties starts from the first column with the 
                      {\tt ColumnType} 'Error' (see example above).
      \item {\tt NInfo}       --- (integer) Calculation of the cross-section predictions may 
                      require  additional information about the data set. The number of 
                      information strings is given by NInfo
      \item {\tt CInfo}       --- (array of strings) Names of the information strings. 
                      Several of them are predefined for different cross-section 
                      calculations.
      \item {\tt DataInfo}    --- (array of float) Values, corresponding to {\tt CInfo} names.
      \item {\tt IndexDataset} -- (integer) Internal H1 Fitter index of the data set. Provide unique
                      numbers to get extra info for $\chi^2/dof$ for each data set.      
      \item {\tt TheoryInfoFile} --- (string) Optional additional theory file with extra 
                     information for cross-section calculation. This could be k-factors,
                     {\tt APPLGRID} file or {\tt FastNLO} table.  
      \item {\tt TheoryType} --- (string) Theory file type ('kfactor', 'applgrid' or 'fastnlo').      
      \item {\tt NKFactor}   --- (integer) For kfactor files, number of columns in
                     {\tt TheoryInfoFile}.
      \item {\tt KFactorNames} --- (array of strings) For kfactor files, names of columns in 
                     {\tt TheoryInfoFile}.
\end{itemize}

Depending on the chosen process specific requirements for the header might be present. 
Dataset-wise options are provided by a {\tt CInfo} / {\tt DataInfo} variable set. In case the information
varies between data points (e.g. bin borders, hadronisation corrections etc.) it is
provided within data table and recognised by the program using reserved column names.
In the following all these requirements are listed and shortly explained.

\subsubsection{Data format requirements for DIS}

In this subsection we describe specific requirements for files using 'NC e+-p' and 'CC e+-p'
reaction types. Examples of such input files are:

{\tt datafiles/H1ZEUS\_NC\_e-p\_HERA1.0.dat}

{\tt datafiles/H1ZEUS\_CC\_e-p\_HERA1.0.dat}.

The properly formatted DIS input files will have the following fields available
in the {\tt CInfo} variable list: 

\begin{itemize} 
    \item  {\tt 'sqrt(S)'} --- the ep collision centre-of-mass energy in GeV. In particular, for 
    HERA based results the the corresponding {\tt DataInfo} value should be $300.$ for measurements
    based on data collected prior to $1997$ (inclusive) and $318.$ afterwards.
    
    \item {\tt 'reduced'} --- a field indicating whether calculated cross section should be reduced (1.) or not (0.)
    (reference to proper equation somewhere in this manual).
    
    \item {\tt 'e charge'} --- electric charge of the collided lepton beam. Supported {\tt DataInfo} values
    are '1.' for electron and '-1.' for positron.

    \item {\tt 'e polarity'} --- polarity of the lepton beam. The corresponding {\tt DataInfo} value 
    should be between $-1.0$ and $1.0$ (is this true?) with abs($1.0$) indicating fully polarised
    beam and $0.0$ fully unpolarised one. 

    In case of non-vanishing polarity following additional fields are required:

    \item {\tt 'pol err unc'} --- explain

    \item {\tt 'pol err corLpol'} --- explain
 
    \item {\tt 'pol err corTpol'} --- explain

\end{itemize}

The inclusive DIS cross sections are calculated on the x-Q2-y grid. Correspondingly,
the following columns need to present in the correctly formatted input file: 
{\tt 'x'}, {\tt 'Q2'} and {\tt 'y'}.


\subsubsection{Data format requirements for FastNLO}

In this subsection we describe data format specific for the FastNLO implementation
accessed by choosing 'FastNLO jets' and 'FastNLO ep jets normalised' reaction types.
Examples of properly formatted files are:

   {\tt datafiles/HERA/ZEUS\_InclJets\_HighQ2\_98-00.dat}

   {\tt datafiles/HERA/H1\_NormInclJets\_HighQ2\_99-07.dat}.

{\tt TheoryType = 'FastNLO'} indicates usage of the FastNLO. The variable {\tt ThoryInfoFile} 
should contain the proper path to the FastNLO table in version 2.0 and higher.
HERAFITTER supports both flexible and inflexible scales.
Older FastNLO tables can be still accessed through the APPLGRID interface.

The following fields are required to be present in the {\tt CInfo} list:

\begin{itemize}

    \item {\tt 'PublicationUnits'} --- The desired units in which the cross sections 
    are calculated by the FastNLO code. If the corresponding {\tt DataInfo} field is 
    set to '1.' the cross sections will be given in the same units as used in the
    relevant publication. In the case it is set to '0.', absolute cross section
    units will be used. 

    \item {\tt 'MurDef', 'MufDef'} --- The renormalisation and factorisation scale definitions
    used with variable scale FastNLO tables. If the chosen FastNLO table does not support 
    variable scales, these fields will be ignored and the scale embedded within the table will 
    be used instead. The values of the corresponding {\tt DataInfo} fields set 
    the renormalisation scale $\mu_r$ and factorisation scale $\mu_f$ following the FastNLO standard:
    \begin{align*} 
       \text{value} :&\quad \text{definition} \\
       0 :&\quad   \mu_{r/f}^2 = \mu_1^2 \\
       1 :&\quad   \mu_{r/f}^2 = \mu_2^2 \\
       2 :&\quad   \mu_{r/f}^2 = ( \mu_1^2 + \mu_2^2 )\\
       3 :&\quad   \mu_{r/f}^2 = ( \mu_1^2 + \mu_2^2 ) / 2 \\
       4 :&\quad   \mu_{r/f}^2 = ( \mu_1^2 + \mu_2^2 ) / 4 \\
       5 :&\quad   \mu_{r/f}^2 = (( \mu_1 + \mu_2 ) / 2 )^2\\
       6 :&\quad   \mu_{r/f}^2 = (( \mu_1 + \mu_2 ))^2\\
       7 :&\quad   \mu_{r/f}^2 = \text{max}( \mu_1^2, \mu_2^2)\\
       8 :&\quad   \mu_{r/f}^2 = \text{min}( \mu_1^2, \mu_2^2) \\
       9 :&\quad   \mu_{r/f}^2 = (\mu_1 * exp(0.3 * \mu_2)) ^2
   \end{align*}

   where $\mu_1$ and $\mu_2$ are specific scales chosen during production of the table. In particular
   for jet production at HERA traditionally 
   \begin{equation*}
          \mu_1^2 = Q^2 \quad \quad \quad \mu_2^2 = p_T^2
    \end{equation*}  

   \item {\tt sqrt(S)} --- Should be defined only for 'FastNLO ep jets normalised' reaction type. 
         The ep collision centre-of-mass energy in GeV. In particular, for 
         HERA based results the the corresponding {\tt DataInfo} value should be $300.$ for measurements
         based on data collected prior to $1997$ (inclusive) and $318.$ afterwards.

   \item {\tt 'lumi(e-)/lumi(tot)'} --- Should be defined only for 'FastNLO ep jets normalised'
         reaction type. The normalisation depends on the ratio of the positron and electron data 
         used for the cross section measurement. This ratio should  be
         given in a format (lumi($e^-$) / (lumi($e^-$) + lumi($e^+$)) and assume values between [0., 1.].

   \item {\tt 'UseZMVFNS'} --- Should be defined for 'FastNLO ep jets normalised' reaction type. The calculation
         of the integrated inclusive DIS cross sections could be time consuming.
         This option provides an opportunity to use a "Zero Mass Variable Flavour
         Number Scheme" approximation which is very fast and possibly provides
         enough precision for the normalisation purposes. ZMVNS is used if 
         the corresponding {\tt DataInfo} field is set to 1. Otherwise, the same scheme
         is used as defined globally with the variable 'HF\_SCHEME' defined in steering.txt file.
\end{itemize}


In addition there are some specific values within the {\tt ColumnName} field which allow
passing the information specific to each data point. They are listed below:

\begin{itemize}
     \item{\tt 'Z0Corr'} --- (optional) The correction due to the $Z_0$ boson exchange.
                 If it is given, each point calculated by the FastNLO code will be
                 multiplied by the {\tt Z0Corr} value.

     \item{\tt 'NPCorr'} --- (optional) The non-perturbative correction.
                 If it is given, each point calculated by the FastNLO code will be
                 multiplied by the {\tt NPCorr} value. {\tt Z0Corr} and {\tt NPCorr} can be added 
                 simultaneously, and in this case the calculated cross sections
                 will be multiplied by the product {\tt Z0Corr} * {\tt NPCorr}.

    \item{\tt 'q2min', 'q2max', 'ymin', 'ymax', 'xmin', 'xmax'} --- Should be defined for 
         'FastNLO ep jets normalised' reaction type and are used to define 
         DIS phase space for the normalisation. Since these three ({\tt q2, y, x}) are 
         connected by the relation
         \begin{equation}
              Q^2 = x \cdot y \cdot s
         \end{equation}
         only two are required to be present to unambiguously define the DIS phase space for each data point.
        
\end{itemize}


%%%%%%%%%%%
\subsection{Selection of Error treatment}

%==========================================
\subsubsection {Offset method}


% -----------------------------------------
{\bf {Correlated error sources}}

The correlated systematic error sources (CSS) are identified by \fitter\ upon reading data files.
Internally, the actual number of CSS 
is stored in the variable \verb'NSYS'
and the maximum allowed number of CSS = \verb'NSYSMAX' (currently 300).
\vspace{0.4cm}

% -----------------------------------------
{\bf {PDFs errors bands}}

The errors bands for the fitted PDFs are calculated 
basing on the full covariance matrix $V = \Vstat + \Vsys$,
and are saved to the 'standard' output files
\verb'pdfs_q2val_*.txt' and \verb'pdfs_*.lhgrid'.
\vspace{0.4cm}

% -----------------------------------------
{\bf {Input parameters}}

File: \verb'steering.txt'

The Offset method is turned on by setting\\
\verb:CHI2Style = 'Offset':

By default all fits are run in a single job, each fit driven by initial parameters and Minuit commands
read from \verb'minuit.in.txt'.

Two optional parameters can be set in the \verb'CSOffset' NAMELIST, e.g.
\vspace*{-2.5ex}
\begin{verbatim}
&CSOffset
  CorSysIndex  =  0
  UsePrevFit = 1
&End
\end{verbatim}
\vspace*{-1ex}
Defaults are set in \verb'read_steer.f'
and the \verb'CSOffset NAMELIST' is read only when the Offset method is active.
\vspace{0.4cm}

% ................................
{\tt CorSysIndex}

Default: \verb'CorSysIndex = NSYSMAX+1'

Setting \verb'CorSysIndex' to any value $\in [-K, K]$ 
restricts the job to a single fit to data shifted (down or up) by a corresponding
correlated error source.
\verb'CorSysIndex' = 0 corresponds to the central fit.

If \verb'CorSysIndex > NSYSMAX' then all the fits are performed.
The best way to perform all fits in a single run is 
to not specify \verb'CorSysIndex' at all.
\vspace{0.4cm}

% ...............................
{\tt UsePrevFit}

This parameter determines how to use results of previous fits,
if such results are present in the \verb'output' folder.

Default: \verb'UsePrevFit' = 0

\begin{enumerate}
\item [0 ---]
Do not use any previous fit results
\item [1 ---]
Use previously obtained parameters as starting values for the current fit.
% For a non-offset fit read initial parameters from \verb'minuit.save.txt';
% for an offset fit 
Read initial parameters from \verb'minuit.save_<CSI>.txt'
 --- e.g.  \verb'minuit.save_001m.txt' for \verb'CorSysIndex' $= -1$.
If the file does not exist and \verb'CorSysIndex' $\neq 0$ try to read
\verb'minuit.save_0.txt'.
\item [2 ---]
Do not perform the fit if a corresponding \verb'Results_<CSI>.txt' file exists,
otherwise switch to mode 1.
\end{enumerate}
\vspace{0.4cm}

% -----------------------------------------
{\bf {Running the fits}}

First, the central fit (\verb'CorSysIndex' = 0) should be thoroughly performed,
resulting in the \verb'minuit.save_0.txt' file with final parameter values.
Next the fits for \verb'CorSysIndex' $\neq 0$
can be run. Setting \verb'UsePrevFit' > 0 will set the initial parameters' values to the `central' ones,
which will speed up the minimisations for non-central fits
and will help to avoid landing in a false minimum.
Eg. setting \verb'UsePrevFit' = 2 and not specifying \verb'CorSysIndex' 
will run all missing fits in one job;
you could run \verb'JayRun.tcl -use 2' (see below).

% Nb. for a job performing all the fits in a single run (\verb'CorSysIndex' unspecified),
% the central fit is done first.

For many CSS one should consider running jobs on some batch system (a `farm').
To this end the code is organised in such a way that in the Offset mode the results of each fit are saved to files
(in the \verb'output' folder). For the central fit the parameters are saved to
\verb'params_0.txt'
and the covariance matrix, $\Vstat$, to \verb'statcov_0.txt';
for other fits, the parameters are saved to
\verb'params_'$j$\verb'{m|p}.txt', for $j = 1,\dots,K$.
Once all results are known the full covariance matrix 
% (Eq. \Eq{eq:Cv-full}) 
is calculated and saved to \verb'offset.save.txt'.
Error bands are calculated if requested in the steering cards.

Actually, each run of the \fitter\ ends with an attempt to perform this final calculation.
If any of the required files is missing the covariance matrix is not calculated
and a warning message is printed.
This behaviour allows for an easy and flexible arrangement of a script-driven parallel computation.
% Running a fit for any {\tt CorSysIndex} always ends with an attempt to collect the final results --- 
% and it succeeds once all results are known.

% We recommend the following sequence:
A typical scenario is:
\begin{enumerate}
\item
run the central fit (\verb'CorSysIndex' = 0),
% --- this makes \verb'minuit.save_0.txt' file with final parameter values,
\item
run fits for \verb'CorSysIndex' $\neq 0$;\\
set \verb'UsePrevFit' = 1
and place \verb'minuit.save_0.txt' in the \verb'output' folder before running the fit,
% --- this will set the initial parameters' values to the `central' ones,
\item
collect all the results in the \verb'output' folder and run the final job with
\verb'UsePrevFit' = 2.
\end{enumerate}

\goodbreak
% ...............................
{\bf {Scripts for batch running}}

A set of example scripts
which facilitate running jobs on a farm is supplied in the \verb'tools/RunJobs' folder.
Configurations for the NQS and ZEUS ZARAH farms are provided.
At the end of this section we give hints on how to customize the scripts for another environment.

\newcommand\HF{\textit{HF}}
In the following
`\HF' denotes your \fitter\ installation folder
and the paths below are relative to this folder.

The scripts must be run from the folder where you run \texttt{FitPDF} locally. 
In other words, the folder must contain input files:\\
\texttt{ewparam.txt},
\texttt{minuit.in.txt},
\texttt{steering.txt},\\
and moreover 
\texttt{datafiles} must be seen as a subfolder. 
Thus \HF\ can be your `run' folder, although I recommend creating a separate folder 
outside the \fitter\ structure, and making there the link\\
\texttt{ln -s \HF/datafiles datafiles}
% \texttt{ln -s \HF/bin bin}

There are three production scripts:
\begin{itemize}
\item \texttt{JayRun.tcl}\\
Runs fit locally for default values of the Offset parameters
(no specific \verb'CorSysIndex' and \verb'UsePrev=0');\\
\verb'JayRun.tcl [-ind <CorSysIndex>] [-use <UsePrev>]'\\
runs according to parameters given.

\item \texttt{JaySub.tcl}\\
Submit all non-central jobs.
The submission will fail if the \verb'output' folder
does not contain results from the central fit.
Identifiers of submitted jobs are written to `JobLog' (\verb'job_work/Last.jobs' file).

\item \texttt{JayGet.tcl}\\
Retrieve jobs. Only jobs from JobLog are looked for.
The number of still running jobs is displayed. 
Each retrieved job is removed from JobLog.
%  job_work/Last.jobs is deleted once there no more running jobs.
There are two useful options, \texttt{peek} and \texttt{run}, e.g.\\
\verb'JayGet.tcl -peek 100 &'\\
runs until all jobs are completed, peeking every 100 seconds and retrieving completed jobs.
It asks for starting the final run once all jobs are done.
If you answer `no', you can later run \verb'JayRun.tcl -ind 0 -use 2'.
This final run can be automated using option \texttt{run}, e.g.\\
\verb'JayGet.tcl -p 300 -run &'\\
which peeks every 300 seconds and performs the final run once all jobs are retrieved.
\end{itemize}

\goodbreak
The configuration options are given in \verb'jay.cfg.tcl'.
This file is read from the folder where the currently called script resides
(\verb'tools/RunJobs' by default).
If a file named \verb'jay.cfg.tcl' exists in the current folder it is read next
--- this provides a method to modify settings locally.

The most important parameters in \verb'jay.cfg.tcl':
\begin{itemize}
\item
\verb'FarmName' --- default = \verb'NQS'\\
The farm specific commands are read from the \texttt{\textit{FarmName}.tcl} file.\\
We supply \verb'NQS.tcl' and \verb'zarah.tcl' containing
routines specific to the corresponding farms.
Note that the ZARAH farm requires a one-time authentication via \verb'zarah-auth',
before starting any activity.

\item
\verb'FarmHost' --- default = "" (empty)\\
The NQS farm host name.

\item
\verb'FarmUser' --- default = "" (empty)\\
The NQS farm user name.

\item
\verb'Driver' --- default = \verb'jay.sh'\\
This `driving script' is sent to the farm as the primary code to execute.
\verb'Driver' is first searched for in the current folder
and next in the folder of the currently called script.
Hence, even without changing this parameter, you can have your own version of the 
driving script (\verb'jay.sh') saved in your current `run' folder.

\item
\verb'MainExe' --- default = \verb'bin/FitPDF'\\
The path to the \fitter\ executable.

\end{itemize}

The two remaining files, \verb'utils.tcl' and \verb'job_farm_fns.tcl', define various utility routines.
They contain no `user serviceable' parts and are independent of a particular farm operation.

In order to add new functionality for some \verb'NewFarm' it should be sufficient to
create a \verb'NewFarm.tcl' file analogous to the supplied
\verb'NQS.tcl' or \verb'zarah.tcl' files, and set proper values of
\verb'FarmName' etc. in your \verb'jay.cfg.tcl' file.

\goodbreak
% ...............................
{\bf {Examples}}

Typically you would proceed as follows:
\begin{enumerate}
\item
run the central fit\\
\HF\verb'/tools/RunJobs/JayRun.tcl -ind 0'\\
or\\
\HF\verb'/tools/RunJobs/JayRun.tcl -ind 0 -use 1'
\item
submit offset fits to the farm\\
\HF\verb'/tools/RunJobs/JaySub.tcl'
\item
collect results and calculate final results\\
\HF\verb'/tools/RunJobs/JayGet.tcl -peek 300 -run'
\end{enumerate}



%%%%%%%%%%%
\subsection{Selection of the data}
  The namelist \&Cuts, located inside the {\tt steering.txt} file can be used to apply
  simple process dependent cuts. The cuts are limitted to bin variables.
  Simple low and high limits are allowed. For example, a cut on $Q^2>3.5$~GeV$^2$ for
  NC ep scattering is specified as

\begin{verbatim}
  ! Rule #1: Q2 cuts
   ProcessName(1)     = 'NC e+-p'
   Variable(1)        = 'Q2'
   CutValueMin(1)     = 3.5 
   CutValueMax(1)     = 1000000.0
\end{verbatim}

  Maximum 100 cuts can be used by default.
%%%%%%%%%%%
\subsection{Understanding the output}
  The results of the minimization are printed to the standard output and written
  to the files in the {\tt output/} directory. 

  The quality of the fit can be judged based on total $\chi^2$ per degrees of freedom.
  It is printed for each iteration as 
\begin{verbatim}
                      Iteration   Chi2   NDF       Chi2/NDF
   FitPDF f,ndf,f/ndf      3      588.64 579        1.02
\end{verbatim}
  The resulting $\chi^2$ is reported at the end of minimisation for each data set and for correlated 
  systematic uncertainties separately. This information is printed and written
  to the {\tt output/Results.txt} file. The {\tt Results.txt} file contains additional 
  information about shifts of the correlated systematic uncertainties.

  The minimization information from the {\tt minuit} program is stored using the standard {\tt minuit} in the {\tt output/minuit.out.txt}
  file. The level of verbosity for this information can be changed by {\tt minuit} commands
  in the {\tt minuit.in.txt} file. Make sure that {\tt minuit} does not report any errors
  or warnings at the end of minimisation.
  
  Point by point comparison of the data and predictions after the minimization 
  is provided in {\tt output/fittedresults.txt} file. The file reports three columns
  corresponding to the three first bins of the input tables, data value, sum in 
  quadrature of statistical and uncorrelated systematic uncertainty, total
  uncertainty, the predicted value, before and after applying correlated systematic shifts,
  pull betweenthe  data and theory and 
  data set index. The pull $p$ is calculated as 
  \begin{equation}
      p = \frac{ \mu - m} {\sigma_{\rm uncor}}
  \end{equation}
  where $\mu$ is the data value, $m$ is the prediction and $\sigma_{\rm uncor}$ is the total
  uncorrelated uncertainty.
  Similar information is stored in the {\tt pulls.first.txt} and {\tt pulls.last.txt} files
  ( dataset index, first bin, second bin, third bin, theory, data, pull).
  Theory is  adjusted for systematic error shifts in this case.

  The output PDFs are stored in  {\tt output/pdfs\_q2val\_XX.txt} files.
  Each of the files reports values of gluon, and quark PDFs as a function of $x$
  for fixed $Q^2$ points. The $Q^2$ values and $x$ grid are specified by 
  {\tt \&Output} namelist in the {\tt steering.txt} file.
  
  The PDF information and data to theory comparisons can be plotted using 
  the {\tt bin/DrawResults} program.  Calling it without arguments plots results from
  {\tt output/} directory. Given the program one argument specifies sub-directory 
  where the information is read. Calling the {\tt bin/DrawResults} program with two
  arguments provides comparison of the PDFs obtained in the two fits.
  
  Finally, the \fitter\ package provides PDFs in the {\tt LHAPDF} format. To obtain the
  {\tt LHAPDF} grid file, run the {\tt tools/tolhapdf.cmd} script. The script produces 
  the {\tt PDFs.LHgrid} file which can be read by the lhapdf version lhapdf-5.8.6.tar.gz
  or later.
%%%%%%%%%%%
\subsection{{\tt Minuit} steering cards}

