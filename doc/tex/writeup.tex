\documentclass[11pt,a4paper]{article}
\usepackage{graphicx,epsfig}
\usepackage{hhline}

\usepackage{amsmath,amssymb}
\usepackage{times}
\usepackage[varg]{txfonts}
\DeclareMathAlphabet{\mathbold}{OML}{txr}{b}{it}

\usepackage{array,multirow,dcolumn}
%\usepackage[mathlines,displaymath]{lineno}
\usepackage{rotating}

 
% we use natbib instead of cite to work with hyperref
%\usepackage{cite}
%\usepackage[numbers,square,comma,sort&compress]{natbib}
%\usepackage{hypernat}
\usepackage{textcomp}

\bibliographystyle{alpha}
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}

% \renewcommand{\arraystretch}{1.2}
\newlength{\dinwidth}
\newlength{\dinmargin}
\setlength{\dinwidth}{21.0cm}
\textheight24cm \textwidth16.0cm
\setlength{\dinmargin}{\dinwidth}
\setlength{\unitlength}{1mm}
\addtolength{\dinmargin}{-\textwidth}
\setlength{\dinmargin}{0.5\dinmargin}
\oddsidemargin -1.0in
\addtolength{\oddsidemargin}{\dinmargin}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\marginparwidth}{0.9\dinmargin}
\marginparsep 8pt \marginparpush 5pt
\topmargin -42pt
\headheight 12pt
\headsep 30pt \footskip 32pt
\parskip 3mm plus 2mm minus 2mm


\newcommand\fitter{ \mbox{\tt HERAFitter} }
\title{\fitter\ - PDF Fitting package}
%\author{H1 Collaboration}
\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
This manual provides a short description of the \fitter\ program 
which can be used to determine unpolarised parton density functions 
(PDFs) using deep inelastic scattering (DIS) data and other processes such as 
Drell-Yan, jet or ttbar processes.
The parton density functions are needed to calculate cross sections
for the $ep$ and $pp$ colliders and thus required for interpetation
of the data collected at the LHC.
% The \fitter\ program were used to determine the HERA1.0 PDF set~\cite{h1zeus:2009wt}.

The manual begins with a brief discussion of the theoretical calculation
used in the program (section~\ref{sec:theory}) followed by description of the
PDF parameterisation (section~\ref{sec:pdf}) and various $\chi^2$ functions used in the
minimisation (section~\ref{sec:chi2}). The installation instructions are given in
section~\ref{sec:install}, depending on various configuration options chose. A description of the program steering cards and
the output options is given in section~\ref{sec:man}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Input}
\label{sec:theory}
The \fitter\ program uses currently as standard the DGLAP~\cite{Gribov:1972ri,Gribov:1972rt,Lipatov:1974qm,Dokshitzer:1977sg,Altarelli:1977zs}
 evolution equations as implemented in the QCDNUM~\cite{qcdnum} program. The fit 
procedure begins with parameterising the input PDFs at the starting 
scale $Q^2_0$ which should be chosen to be below the charm mass threshold
$m_C^2$.
The PDFs are then evolved using the DGLAP evolution equations  
at NLO~\cite{Curci:1980uw,Furmanski:1980cm} in the $\overline{MS}$ scheme.
The renormalisation and factorisation scales are set to $Q^2$. The \fitter\ program
also allows for LO and NNLO evolution. 

The cross-section predictions are obtain by convoluting the PDFs with the 
hard scattering coefficient functions. For the DIS processes, those are calculated 
using the general mass variable-flavour scheme. 
The program implements the  zero mass scheme from QCDNUM as well as
various treatments for the heavy quark thresholds as provided by the MSTW group
- the RT scheme with its variants at NLO and NNLO ~\cite{Thorne:1997ga,Thorne:2006qt}, as provided by the CTEQ group - the ACOT scheme with its variants at LO and NLO, as provided by the ABM group - the BMSN scheme at NLO and NNLO.
Each of these schemes is briefly discussed in further details.

The jet cross sections
are calculated using APPLGRID and FastNLO. The program has two implementations
for $pp$  DY processes. The first implementation uses
calculations at LO which can be extended to NLO using k-factors,
the second uses the APPLGRID interface.
For thorough details of the theoretical modules we direct the user to read the provided  references of these packages.


%%%%%%%%%%%
\subsection{Flavour Schemes}
%%%%
\subsubsection{ZMVFNS}
%%%%
\subsubsection{RT}
... 
%%%%
\subsubsection{ACOT}
...
%%%%
\subsubsection{ABM}
...
%%%%%%%%%%%
\subsection{$ep$ Electroweak corrections via HS}
%%%%%%%%%%%
\subsection{DY process}
%%%%
\subsubsection{Leading Order}
%%%%
\subsubsection{Next Leading Order}
%%%%%%%%%%%
\subsection{$t\bar{t}$ Cross Sections via Hathor}
%%%%%%%%%%%
\subsection{Jets}
%%%%
\subsubsection{FastNLO}
%%%%
\subsubsection{APPLGRID}
%%%%%%%%%%%
\subsection{DIPOLE models}
%%%%
\subsubsection{GBW model}
%%%%
\subsubsection{IIM model}
%%%%
\subsubsection{BGK model}
%%%%
\subsubsection{Mixed with DGLAP model}
%%%%%%%%%%%
\subsection{Unintegrated PDFs using CASCADE}
%%%%%%%%%%%
\subsection{Diffractive DIS PDFs}
Diffractive DIS data are fitted within the 'proton vertex factorisation' approach where 
the diffractive DIS is mediated by the exchange of hard Pomeron and a secondary Reggeon.
The model supplied by the DiffDIS package provides values of the 'reduced cross section',
$\sigma_r = F_2 - y^2/(1+(1-y)^2) F_L$
which is expected to be the experimentally meausured quantity.
The model supplied by the DiffDIS package provides values of the 'reduced cross section',
$\sigma_r = F_2 - y^2/(1+(1-y)^2) F_L$
which is expected to be the experimentally meausured quantity.

%%%%%%%%%%%
\subsection{Reweighting Techniques}
%%%%
\subsubsection{Using Monte Carlo method as in NNPDF}
This module should be renamed as it is a bit misleading.
This modules refers to the reweighting technique developed
by the NNPDF collaboration in the context of the PDF determinations.
The NNPDF reweighting calculates the $\chi^2$ between a new data set and the old NNPDF replicas in order to determine which replicas are still able to describe the new data (they are kept) and which ones fail (they are thrown out).

The output of the procedure is a new, updated NNPDF set in LHAPDF format with a reduced number of replicas that describe the old and the new data well. 
Some additional check plots which give clues about the validity of the procedure for the given new data set are also provided.
%%%%
\subsubsection{using Eigenvector reweighting as in MSTW}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PDF Parameterisation}
\label{sec:pdfparam}
%%%%%%%%%%%
\subsection{Standard Functional form}
%%%%
\subsubsection{CTEQ style}
%%%%
\subsubsection{HERAPDF style}
%%%%
\subsubsection{Flexible style}
%%%%%%%%%%%
\subsection{Chebyshev Polynomial}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\chi^2$ Definitions}
\label{sec:chi2}
%%%%%%%%%%%
\subsection{Using Nuisance Parameters}
%%%%
\subsubsection{Simple Form}

%%%%
\subsubsection{Scaled Form}

For a single data set, 
 the $\chi^2$ function can be defined as~\cite{H1:2009bp}
%
\begin{equation}
 \chi^2_{\rm exp}\left(\boldsymbol{m},\boldsymbol{b}\right) = %\\
%~~~=
 \sum_i
 \frac{\left[m^i
- \sum_j \gamma^i_j m^i b_j  - {\mu^i} \right]^2}
{ \textstyle \delta^2_{i,{\rm stat}}\left(m^i -  \sum_j \gamma^i_j m^i b_j\right)+
\left(\delta_{i,{\rm uncor}}\,  m^i\right)^2}
 + \sum_j b^2_j.
\label{eq:ave}\end{equation}
%
Here ${\mu^i}$ is the  measured central value  at a point $i$ 
with  relative statistical $\delta_{i,stat}$ 
and relative uncorrelated systematic uncertainty $\delta_{i,unc}$.
Further, $\beta_j$ denotes a nuisance parameter for
 a correlated systematic error  source of type $j$ with an uncertainty
 while
$\gamma^i_j$ 
quantifies the sensitivity of the
measurement ${\mu^i}$ at the point $i$ to the systematic source $j$. 
The function $\chi^2_{\rm exp}$ depends on the set of
underlying physical quantities $m^i$ 
(denoted as the vector $\boldsymbol{m}$) and 
 the set of systematic uncertainties $b_j$ ($\boldsymbol{b}$).
This definition of the $\chi^2$ function takes into account that
systematic uncertainties are proportional to the central values 
(multiplicative errors), whereas the statistical errors scale 
with the square roots of the expected number of events. 
Other scaling properties for the statistical and uncorrelated
systematic uncertainties are available as described in appendix~\ref{sec:herafitter}.
%%%%
\subsubsection{Generalised Scaled Form}

%%%%%%%%%%%
\subsection{Using Covariance Matrix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Uncertainties}
%%%%%%%%%%%
\subsection{Hessian Method}

%%%%%%%%%%%
\subsection{Monte Carlo Method}
%%%%%%%%%%%
\subsection{Regularisation methods}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program Installation Instructions} 
\label{sec:install}
%%%%%%%%%%%

The Installation Instructions are dependent on which modules are activated via the configuration option. 
\subsection{Pre-requirements}

The following packages are needed in order to build \fitter\ package:
\begin{itemize}
\item QCDNUM~\cite{qcdnum} version at least {\tt qcdnum-17-00/04}, can be found at \\
  {\tt http://mbotje.web.cern.ch/mbotje/qcdnum/Site/QCDNUM17.html}
\item {\tt CERNLIB} libraries. Note that for {\tt CERNLIB} one can use {\tt /afs/} installation from CERN:
  {\tt /afs/cern.ch/sw/lcg/external/cernlib/}
%\item Link to recent Root libraries (e.g. version 5.26)
%\item Optional: {\tt APPLGRID}
\end{itemize}
The \fitter\ program has been tested on various platforms: 
   SL4, SL5 (32 and 64 bit),  Ubuntu 10.10.
%%%%%%%%%%%
\subsection{Default Installation}
\begin{itemize}
\item
 Specify {\tt CERN\_ROOT} 
     and {\tt QCDNUM\_ROOT} variables such that 
     {\tt \$CERN\_ROOT/lib}  and {\tt \$QCDNUM\_ROOT/lib}
 point to the corresponding libraries
\item Run:
\begin{verbatim}
%    autoreconf --install
    ./configure
    make 
    make install
\end{verbatim}
After these commands are finished, the executable {\tt bin/FitPDF} 
file should be installed
\item  Run a check:
\begin{verbatim}
    bin/FitPDF 
\end{verbatim}
\end{itemize}
%%%%%%%%%%%
\subsection{Installation with {\tt APPLGRID}}
\begin{itemize}
\item
 Specify {\tt CERN\_ROOT} and {QCDNUM\_ROOT} variables such that 
     {\tt \$CERN\_ROOT/lib}  and {\tt \$QCDNUM\_ROOT/lib}
 point to the corresponding libraries
\item Make sure that {\tt \$PATH} and {\tt \$LD\_LIBRARY\_PATH} 
variables point to the {\tt APPLGRID} environment.
\item Run:
\begin{verbatim}
    autoreconf --install
    ./configure --enable-applgrid
    make 
    make install
\end{verbatim}
After these commands are finished, the executable {\tt bin/FitPDF} 
file should be installed
\item  Run a check:
\begin{verbatim}
    bin/FitPDF 
\end{verbatim}
\end{itemize}
%%%%%%%%%%%
\subsection{Installation with {\tt LHAPDF}}
%%%%%%%%%%%
\subsection{Installation with {\tt NNPDF reweight}}
%%%%%%%%%%%
\subsection{Installation with {\tt HATHOR}}
%%%%%%%%%%%
\subsection{Installation with {\tt CASCADE}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Short Program Manual}
\label{sec:man}
%%%%%%%%%%%
\subsection{Steering files}
    The software behaviour is controlled by two files with steering commands.
    These files have predefined names:
    \begin{itemize}
      \item {\tt steering.txt}  --   controls main "stable" (un-modified during 
                         minimisation) parameters. The file also contains
                         names of data files to be fitted to, definition 
                         of kinematic cuts                              
      \item {\tt minuit.in.txt}
                   --  controls minimisation parameters and minimisation 
                         strategy. Standard Minuit commands can be provided
                         in this file
      \item {\tt ewparam.txt}    --  controls electroweak parameters such
         as W and Z boson masses and CKM matrix parameters.
    \end{itemize}
%%%%%%%%%%%
\subsection{Inclusion data in the fit}
Inclusion of the data files is controlled by {\tt \&InFiles} namelist in the 
{\tt steering.txt} file. For example, by default the following four HERA-I
    files are included:
\begin{verbatim}
&InFiles
    NInputFiles = 4
    InputFileNames(1) = 'datafiles/H1ZEUS_NC_e-p_HERA1.0.dat'
    InputFileNames(2) = 'datafiles/H1ZEUS_NC_e+p_HERA1.0.dat'
    InputFileNames(3) = 'datafiles/H1ZEUS_CC_e-p_HERA1.0.dat'
    InputFileNames(4) = 'datafiles/H1ZEUS_CC_e+p_HERA1.0.dat'
&End
\end{verbatim}

To include more files:
\begin{itemize}
 \item  Increase the {\tt NInputFiles} variable.
 \item  Specify the additional file by providing corresponding
  {\tt InputFileNames()} variable.
\end{itemize}
Details about data file format can be found in section~\ref{sec:dataformat}.
%%%%%%%%%%%
\subsection{Data file format}
\label{sec:dataformat}
   Experimental data are provided by the standard {\tt ASCII} text files. The files
   contain a "header" which describes the data format and the "data" in terms
   of a 2-dimensional table. Each line of the data table corresponds to a
   data point, the meaning of the columns is specified in the file header.

   For example, a header for HERA-I combined H1-ZEUS data for e+p neutral 
   current scattering cross section is given in the file

\begin{verbatim}
       datafiles/H1ZEUS_NC_e-p_HERA1.0.dat
\end{verbatim}

   The format of the file follows standard "namelist" conventions. Comments 
   start with exclamation mark.  Pre-defined variables are:
\begin{itemize}
     \item{\tt Name}        --- (string) provides a name of the data set
    \item{\tt  Reaction}    --- (string) reaction type of the data set. Reaction type is used 
                      to trigger corresponding theory calculation. The following 
                      reaction types  are currently supported by the HERAFitter:
                      \begin{itemize}
                        \item {\tt 'NC e+-p'}  -- double differential NC ep scattering
                                      (ZMVFS and RT-VFS schemes) 
                        \item {\tt 'CC e+-p'}  -- double differential CC ep scattering
                                      (ZMVFS scheme)
                        \item {\tt 'CC pp'}    -- single differential $d \sigma_{W^{\pm}}/d eta_{\ell^{\pm}}$
                                      production and W asymmetry at $pp$ and $p\bar{p}$ 
                                      colliders (LO+kfactors and APPLGRID interface)
                        \item {\tt 'NC pp'}    -- single differential $d \sigma_Z / d y_Z$ at $pp$ and
                                      $p\bar{p}$ colliders
                                      ({\tt LO} with k-factors and {\tt APPLGRID} interface)

                        \item 'pp jets APPLGRID' -- $pp\to$ inclusive jet production, using
                                     {\tt APPLGRID}
                      \end{itemize}                       
      \item {\tt NData}       --- (integer) specifies number of data points in the file. 
                     This corresponds to the number of table rows which 
                     follow after the header.
      \item {\tt NColumn}     --- (integer) number of columns in the data table.
      \item {\tt ColumnType}  --- (array of strings)
                      Defines layout of the data table. The following column types
                      are pre-defined: 'Bin', 'Sigma', 'Error' and 'Dummy'
                      The keywords are case sensitive. 'Bin' correspond to an
                      abstract bin definition, 'Sigma' corresponds to the data
                      measurement, 'Error' - to various type of uncertainties and
                      'Dummy' indicates that the column should be ignored.
      \item {\tt ColumnName}  --- (array of strings)
                      Defines names of the columns. The meaning of the name depends
                      on the ColumnType. For ColumnType 'Bin', ColumnName gives a
                      name of the abstract bin. The abstract bins can contain
                      any variable names, but some of them must be present for 
                      correct cross section calculation. For example, 'x', 'Q2' and
                      'y' are required for DIS NC cross-section calculation.
 
                      For ColumnType 'Sigma', ColumnName provides a label for 
                      the observable, which can be any string.
 
                      For ColumnType 'Error', the following names have special meaning:
                      \begin{itemize}
                       \item 'stat'  -- specifies column with statistical uncertainties;
                       \item 'uncor' -- specifies column with uncorrelated uncertainties;  
                       \item 'total' -- specifies column with total uncertainties. 
                                  Total uncertainties are not used in the fit,
                                  however there is an additional check is performed
                                  if 'total' column is specified: sum in quadrature
                                  of statistical, uncorrelated and correlated 
                                  systematic uncertainties is compared to the total
                                  and a warning is issued if they differ significantly.
                       \item'ignore' - specifies column to be ignored (for special studies).
                       \item Other names specifies columns of correlated systematic 
                      uncertainty. For a given data file, each column of the correlated
                      uncertainty must have unique name. To specify correlation across
                      data files, same name must be used for different files.  
                      \end{itemize}
      \item {\tt SystScales}  --- (array of float)
                      For special studies, systematic uncertainties can be scaled
                      The numbering of uncertainties starts from the first column
                      with the ColumnType 'Error'. For example, setting 
\begin{verbatim}
                  SystScale(1) = 2.  
\end{verbatim}
                      in {\tt datafiles/H1ZEUS\_NC\_e-p\_HERA1.0.dat} would scale stat. 
                      uncertainty by factor of two.                       
      \item {\tt Percent}     --- (array of bool) For each uncertainty specify if it is given in 
                      absolute ("false") or in percent ("true").  The numbering of 
                      uncertainties starts from the first column with the 
                      {\tt ColumnType} 'Error' (see example above).
      \item {\tt NInfo}       --- (integer) Calculation of the cross-section predictions may 
                      require  additional information about the data set. The number of 
                      information strings is given by NInfo
      \item {\tt CInfo}       --- (array of strings) Names of the information strings. 
                      Several of them are predefined for different cross-section 
                      calculations.
      \item {\tt DataInfo}    --- (array of float) Values, corresponding to {\tt CInfo} names.
      \item {\tt IndexDataset} -- (integer) Internal H1 Fitter index of the data set. Provide unique
                      numbers to get extra info for $\chi^2/dof$ for each data set.      
      \item {\tt TheoryInfoFile} --- (string) Optional additional theory file with extra 
                     information for cross-section calculation. This could be k-factors,
                     {\tt APPLGRID} file or {\tt FastNLO} table.  
      \item {\tt TheoryType} --- (string) Theory file type ('kfactor', 'applgrid' or 'fastnlo').      
      \item {\tt NKFactor}   --- (integer) For kfactor files, number of columns in
                     {\tt TheoryInfoFile}.
      \item {\tt KFactorNames} --- (array of strings) For kfactor files, names of columns in 
                     {\tt TheoryInfoFile}.
\end{itemize}

%%%%%%%%%%%
\subsection{Selection of the data}
  The namelist \&Cuts, located inside the {\tt steering.txt} file can be used to apply
  simple process dependent cuts. The cuts are limitted to bin variables.
  Simple low and high limits are allowed. For example, a cut on $Q^2>3.5$~GeV$^2$ for
  NC ep scattering is specified as

\begin{verbatim}
  ! Rule #1: Q2 cuts
   ProcessName(1)     = 'NC e+-p'
   Variable(1)        = 'Q2'
   CutValueMin(1)     = 3.5 
   CutValueMax(1)     = 1000000.0
\end{verbatim}

  Maximum 100 cuts can be used by default.
%%%%%%%%%%%
\subsection{Understanding the output}
  The results of the minimization are printed to the standard output and written
  to the files in the {\tt output/} directory. 

  The quality of the fit can be judged based on total $chi^2$ per degrees of freedom.
  It is printed for each iteration as 
\begin{verbatim}
                      Iteration   Chi2   NDF       Chi2/NDF
   FitPDF f,ndf,f/ndf      3      588.64 579        1.02
\end{verbatim}
  The resulting $chi^2$ is reported at the end of minimisation for each data set and for correlated 
  systematic uncertainties separately. This information is printed and written
  to the {\tt output/Results.txt} file. The {\tt Results.txt} file contains additional 
  information about shifts of the correlated systematic uncertainties.

  The minimization information from the {\tt minuit} program is stored using the standard {\tt minuit} in the {\tt output/minuit.out.txt}
  file. The level of verbosity for this information can be changed by {\tt minuit} commands
  in the {\tt minuit.in.txt} file. Make sure that {\tt minuit} does not report any errors
  or warnings at the end of minimisation.
  
  Point by point comparison of the data and predictions after the minimization 
  is provided in {\tt output/fittedresults.txt} file. The file reports three columns
  corresponding to the three first bins of the input tables, data value, sum in 
  quadrature of statistical and uncorrelated systematic uncertainty, total
  uncertainty, the predicted value, before and after applying correlated systematic shifts,
  pull betweenthe  data and theory and 
  data set index. The pull $p$ is calculated as 
  \begin{equation}
      p = \frac{ \mu - m} {\sigma_{\rm uncor}}
  \end{equation}
  where $\mu$ is the data value, $m$ is the prediction and $\sigma_{\rm uncor}$ is the total
  uncorrelated uncertainty.
  Similar information is stored in the {\tt pulls.first.txt} and {\tt pulls.last.txt} files
  ( dataset index, first bin, second bin, third bin, theory, data, pull).
  Theory is  adjusted for systematic error shifts in this case.

  The output PDFs are stored in  {\tt output/pdfs\_q2val\_XX.txt} files.
  Each of the files reports values of gluon, and quark PDFs as a function of $x$
  for fixed $Q^2$ points. The $Q^2$ values and $x$ grid are specified by 
  {\tt \&Output} namelist in the {\tt steering.txt} file.
  
  The PDF information and data to theory comparisons can be plotted using 
  the {\tt bin/DrawResults} program.  Calling it without arguments plots results from
  {\tt output/} directory. Given the program one argument specifies sub-directory 
  where the information is read. Calling the {\tt bin/DrawResults} program with two
  arguments provides comparison of the PDFs obtained in the two fits.
  
  Finally, the \fitter\ package provides PDFs in the {\tt LHAPDF} format. To obtain the
  {\tt LHAPDF} grid file, run the {\tt tools/tolhapdf.cmd} script. The script produces 
  the {\tt PDFs.LHgrid} file which can be read by the lhapdf version lhapdf-5.8.6.tar.gz
  or later.
%%%%%%%%%%%
\subsection{{\tt Minuit} steering cards}

%%%%%%%%%%%%%%%%%%%%%%%%%
\section{User Examples}
%%%%
\subsection{DIS inclusive only}

%%%%
\subsection{All processes}


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{writeup.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{{\tt \&HERAFitter} namelist format}
\label{sec:herafitter}
\begin{itemize}
  \item {\tt ITheory} --- (integer) Currently only QCDNUM standard evolution
     is implemented for which {\tt ITheory} is set to 0.
  \item {\tt IOrder} --- (integer) For {\tt ITheory} =0 (collinear factorisation) : 
        LO fit (1) or NLO (2) or NNLO (3) 
  \item {\tt Q02} --- (float) Evolution starting scale.
  \item {\tt HF\_SCHEME} --- Specify heavy quark flavour treatment for neutral
 current $ep$ process. The following schemes are implemented: 
    \begin{itemize}
      \item {\tt 'ZMVFNS'}: Zero Mass Variable Flavour Number Scheme, as implemented
 in {\tt QCDNUM}.
      \item {\tt 'RT'}: Thorne-Roberts VFN scheme for $F_2^{\gamma}$. 
      \item {\tt 'RT FAST'}: Fast approximate RT VFN scheme using k-factor 
with respect ot QCDNUM ZMVFNS, calculated at the first iteration.
    \end{itemize}
\item {\tt PDFStyle} --- (string) PDF parameterisation style. Possible styles are currently available:
   \begin{itemize}
  \item{\tt '10p HERAPDF'} -- HERAPDF-like with an extra assumption 
                                 $B_{u_v} = B_{d_v}$;
  \item{\tt '13p HERAPDF'} -- HERAPDF-like with $B_{u_v}$ and $B_{d_v}$ 
                          floated independently;
  \item{\tt '10p H12000'}  -- H12000-like with independent PDFs being the
               $D,U,\bar{D},\bar{U}$ quarks and gluon.
  \item{\tt 'CTEQ'}        -- CTEQ-like parameterisation.
  \item{\tt 'CHEB'}        -- CHEBYSHEV parameterisation based on 
         gluon,sea, $u_{v}$, $d_{v}$ independent pdfs.
 \end{itemize}
\item {\tt CHI2Style}  --- (string) choice of the $\chi^2$ function:
   \begin{itemize}
   \item {\tt 'H12000'} -- Pascaud-like, systematic shifts to theory, no scaling of statistical, uncorrelated errors.
   \item {\tt 'HERAPDF'} -- Pascaud-like + "mixed error scaling"
   \item {\tt 'HERAPDF Sqrt'}   -- Pascaud-like + "sqrt error scaling"
   \item {\tt 'HERAPDF Linear'} -- Pascaud-like + "linear error scaling"
 \end{itemize}
  \item {\tt LDEBUG}  --- (logical) debug flag.
\end{itemize}

\end{document}

