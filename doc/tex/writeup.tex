\documentclass[11pt,a4paper]{article}
\usepackage{graphicx,epsfig}
\usepackage{hhline}

\usepackage{amsmath,amssymb}
\usepackage{times}
\usepackage[varg]{txfonts}
\DeclareMathAlphabet{\mathbold}{OML}{txr}{b}{it}

\usepackage{array,multirow,dcolumn}
%\usepackage[mathlines,displaymath]{lineno}
\usepackage{rotating}

 
% we use natbib instead of cite to work with hyperref
%\usepackage{cite}
%\usepackage[numbers,square,comma,sort&compress]{natbib}
%\usepackage{hypernat}
\usepackage{textcomp}

\bibliographystyle{alpha}
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}

% \renewcommand{\arraystretch}{1.2}
\newlength{\dinwidth}
\newlength{\dinmargin}
\setlength{\dinwidth}{21.0cm}
\textheight24cm \textwidth16.0cm
\setlength{\dinmargin}{\dinwidth}
\setlength{\unitlength}{1mm}
\addtolength{\dinmargin}{-\textwidth}
\setlength{\dinmargin}{0.5\dinmargin}
\oddsidemargin -1.0in
\addtolength{\oddsidemargin}{\dinmargin}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\marginparwidth}{0.9\dinmargin}
\marginparsep 8pt \marginparpush 5pt
\topmargin -42pt
\headheight 12pt
\headsep 30pt \footskip 32pt
\parskip 3mm plus 2mm minus 2mm


\newcommand\fitter{ \mbox{\tt HERAFitter} }
\title{\fitter\ - PDF Fitting package}
%\author{H1 Collaboration}
\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
This manual provides a short description of the \fitter\ program 
which can be used to determine unpolarised parton density functions 
(PDFs) using deep inelastic scattering (DIS) data and other processes such as 
Drell-Yan, jet or ttbar processes.
The parton density functions are needed to calculate cross sections
for the $ep$ and $pp$ colliders and thus required for interpetation
of the data collected at the LHC.
% The \fitter\ program were used to determine the HERA1.0 PDF set~\cite{h1zeus:2009wt}.

The manual begins with a brief discussion of the theoretical calculation
used in the program (section~\ref{sec:theory}) followed by description of the
PDF parameterisation (section~\ref{sec:pdfparam}) and various $\chi^2$ functions used in the
minimisation (section~\ref{sec:chi2}). The installation instructions are given in
section~\ref{sec:install}, depending on various configuration options chose. A description of the program steering cards and
the output options is given in section~\ref{sec:man}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program Installation Instructions} 
\label{sec:install}
%%%%%%%%%%%

The Installation Instructions are dependent on which modules are activated via the configuration option. 
\subsection{Pre-requirements}

The following packages are needed in order to build \fitter\ package:
\begin{itemize}
\item QCDNUM~\cite{qcdnum} version at least {\tt qcdnum-17-00/04}, can be found at \\
  {\tt http://mbotje.web.cern.ch/mbotje/qcdnum/Site/QCDNUM17.html}
\item {\tt CERNLIB} libraries. Note that for {\tt CERNLIB} one can use {\tt /afs/} installation from CERN:
  {\tt /afs/cern.ch/sw/lcg/external/cernlib/}
%\item Link to recent Root libraries (e.g. version 5.26)
%\item Optional: {\tt APPLGRID}
\end{itemize}
The \fitter\ program has been tested on various platforms: 
   SL4, SL5 (32 and 64 bit),  Ubuntu 10.10.
%%%%%%%%%%%
\subsection{Default Installation}
\begin{itemize}
\item
 Specify {\tt CERN\_ROOT} 
     and {\tt QCDNUM\_ROOT} variables such that 
     {\tt \$CERN\_ROOT/lib}  and {\tt \$QCDNUM\_ROOT/lib}
 point to the corresponding libraries
\item Run:
\begin{verbatim}
%    autoreconf --install
    ./configure
    make 
    make install
\end{verbatim}
After these commands are finished, the executable {\tt bin/FitPDF} 
file should be installed
\item  Run a check:
\begin{verbatim}
    bin/FitPDF 
\end{verbatim}
\end{itemize}
%%%%%%%%%%%
\subsection{Installation with {\tt APPLGRID}}
\begin{itemize}
\item
 Specify {\tt CERN\_ROOT} and {QCDNUM\_ROOT} variables such that 
     {\tt \$CERN\_ROOT/lib}  and {\tt \$QCDNUM\_ROOT/lib}
 point to the corresponding libraries
\item Make sure that {\tt \$PATH} and {\tt \$LD\_LIBRARY\_PATH} 
variables point to the {\tt APPLGRID} environment.
\item Run:
\begin{verbatim}
    autoreconf --install
    ./configure --enable-applgrid
    make 
    make install
\end{verbatim}
After these commands are finished, the executable {\tt bin/FitPDF} 
file should be installed
\item  Run a check:
\begin{verbatim}
    bin/FitPDF 
\end{verbatim}
\end{itemize}
%%%%%%%%%%%
\subsection{Installation with {\tt LHAPDF}}
%%%%%%%%%%%
\subsection{Installation with {\tt NNPDF reweight}}
%%%%%%%%%%%
\subsection{Installation with {\tt HATHOR}}
%%%%%%%%%%%
\subsection{Installation with {\tt CASCADE}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Input}
\label{sec:theory}
The \fitter\ program uses currently as standard the DGLAP~\cite{Gribov:1972ri,Gribov:1972rt,Lipatov:1974qm,Dokshitzer:1977sg,Altarelli:1977zs}
 evolution equations as implemented in the QCDNUM~\cite{qcdnum} program. The fit 
procedure begins with parameterising the input PDFs at the starting 
scale $Q^2_0$ which should be chosen to be below the charm mass threshold
$m_C^2$.
The PDFs are then evolved using the DGLAP evolution equations  
at NLO~\cite{Curci:1980uw,Furmanski:1980cm} in the $\overline{MS}$ scheme.
The renormalisation and factorisation scales are set to $Q^2$. The \fitter\ program
also allows for LO and NNLO evolution. 

The cross-section predictions are obtain by convoluting the PDFs with the 
hard scattering coefficient functions. For the DIS processes, those are calculated 
using the general mass variable-flavour scheme. 
The program implements the  zero mass scheme from QCDNUM as well as
various treatments for the heavy quark thresholds as provided by the MSTW group
- the RT scheme with its variants at NLO and NNLO ~\cite{Thorne:1997ga,Thorne:2006qt}, as provided by the CTEQ group - the ACOT scheme with its variants at LO and NLO, as provided by the ABM group - the BMSN scheme at NLO and NNLO.
Each of these schemes is briefly discussed in further details.

The jet cross sections
are calculated using APPLGRID and FastNLO. The program has two implementations
for $pp$  DY processes. The first implementation uses
calculations at LO which can be extended to NLO using k-factors,
the second uses the APPLGRID interface.
For thorough details of the theoretical modules we direct the user to read the provided  references of these packages.


%%%%%%%%%%%
\subsection{DIS and Schemes}
%%%%
\subsubsection{ZMVFNS}
%%%%
\subsubsection{RT}
... 
%%%%
\subsubsection{ACOT}
...
%%%%
\subsubsection{ABM}
An interface to open-source code OPENQCDRAD~\cite{openqcdrad_page} in HERAFitter framework 
provides an access to 
fixed flavour number scheme (FFNS)~\cite{Laenen:1992,Laenen:1993,Riem:1995}~\footnote{Besides 
the variable flavour number 
scheme, QCDNUM also supports the fixed flavour number scheme which can be used for variouse 
cross checks with OPENQCDRAD.}. In FFNS the number of light quark 
flavours $n_{f}$ (here $n_{f}=3$) are considered in the PDF evolution and heavy (massive) 
quarks appear only in the final state. 
The QCD corrections to the massive Wilson coefficients which are known up to the NNLO
for the neutral-current (NC) heavy-quark production~\cite{} are implemented in OPENQCDRAD.
In the case of charged-current (CC), the massive NLO QCD corrections~\cite{} are available.
In addition, the treatment of the heavy-quark contributions in DIS are provided 
in both, the pole-mass and the running-mass definition in $\overline{\text{MS}}$ 
scheme~\cite{Alekhin:runm}. \\
In case the FFN scheme is chosen as the fitting option (see corresponding instructions in the section 1),
the heavy quark contributions to DIS structure functions $F_2$ and $F_L$ (and $F_3$ in the charged 
current case) are calculated in the FFNS and together with the light-flavor contributions are 
provided for the theory prediction calculation (theory$\_$dispatcher.f).
The interpolation to PDFs and $\alpha_s$ evolution from QCDNUM are set up in the interface to OPENQCDRAD. \\
The variation of the renormalisation and factorisation scales for heavy quarks is 
possible (see available options in steering.txt file).
%
%       
%%%%%%%%%%%
\subsubsection{$ep$ Electroweak corrections via HS}
%%%%%%%%%%%
\subsection{DY process}
%%%%
\subsubsection{Leading Order}
The leading order Drell-Yan~\cite{Drell:1970wh,Yamada:1981mw} cross section 
for the neutral current, triple differential in
invariant mass \(M\), boson rapidity \(y\) and CMS
lepton scattering angle \(\cos\theta\), can be written as
\begin{align}
\frac{\mathrm{d}^3\sigma}{\mathrm{d}M\mathrm{d}y\mathrm{d}\cos\theta} &=
  \frac{\pi\alpha^2}{3MS}\sum_{q}P_q
  \left[F_q(x_1,Q^2)F_{\bar{q}}(x_2,Q^2) + (q\leftrightarrow\bar{q})\right],
\end{align}
where \(S\) is a squared CMS beam energy, \(x_{1,2} = \frac{M}{\sqrt{S}}\exp(\pm y)\) and 
\begin{align}
  P_q &=  e_l^2e_q^2(1+\cos^2\theta) \nonumber \\
      &+  e_le_q\frac{2M^2(M^2-M_Z^2)}{\sin^2\theta_W\cos^2\theta_W
          \big[(M^2-M_Z^2)^2+\Gamma_Z^2M_Z^2\big]}
          \big[aA_q(1+\cos^2\theta)+2bB_q\cos\theta\big] \nonumber \\
      &+  \frac{M^4}{\sin^4\theta_W\cos^4\theta_W
          \big[(M^2-M_Z^2)^2+\Gamma_Z^2M_Z^2\big]}
          \big[(a^2+b^2)(A_q^2+B_q^2)(1+\cos^2\theta)+8abA_qB_q\cos\theta\big].
\end{align}
Here \(\theta_W\) is the Weinberg angle, \(M_Z\) and \(\Gamma_Z\) are Z boson mass and 
width, and
\begin{align}
 a & = -\frac{1}{4} + \sin^2\theta_W,  \nonumber \\
 b & = -\frac{1}{4},  \nonumber \\
 A_q & = \frac{1}{2}I_q^3-e_q\sin^2\theta_W, \nonumber \\
 B_q & = \frac{1}{2}I_q^3,  \nonumber \\
 I_u^3 & = -I_d^3 = \frac{1}{2},  \nonumber \\
 e_l & = -1, e_u = \frac{2}{3}, e_d = -\frac{1}{3}.
\end{align}

The expression for charged current has simpler form:
\begin{align}
\frac{\mathrm{d}^3\sigma}{\mathrm{d}M\mathrm{d}y\mathrm{d}\cos\theta} &=
 \frac{\pi\alpha^2}{48S\sin^4\theta_W}
 \frac{M^3(1-\cos\theta)^2}{(M^2-M_W^2)+\Gamma_W^2M_W^2}
 \sum_{q_1,q_2}V_{q_1q_2}^2F_{q_1}(x_1,Q^2)F_{q_2}(x_2,Q^2),
\end{align}
where \(V_{q_1q_2}\) is the CKM quark mixing matrix and \(M_W\) and \(\Gamma_W\)
are \(W\) boson mass and decay width.

The simple form of these expressions allows to calculate integrated
cross sections without utilization of Monte-Carlo techniques.
This is particularly useful for PDF fitting purposes because
the statistical fluctuations are avoided in this case. In both 
neutral and charge current expressions the parton density functions
factorize as a function dependent only on boson rapidity \(y\) and
invariant mass \(M\) leaving \(\cos\theta\) dependence aside.
The integral in \(\cos\theta\) can be computed analytically and
integrations in \(y\) and \(M\) can be performed with Simpson
method. The \(\cos\theta\) parts are kept in the equation 
explicitly because their integration is asymmetric for
data in lepton \(\eta\) bins and also is being performed when applying 
the lepton \(p_{\perp}\) cuts.

The fact that PDF functions factorize with the rest part of 
expression allows to significantly boost calculations when 
performing parameter fits over lepton rapidity data. In this case
the factorized part of expression independent on PDFs can be
calculated only once for all minimization iterations.
The leading order code in HERAFitter package implements this 
optimization and uses fast convolution routines provided by
QCDNUM. Currently the full width LO calculations are optimized 
for lepton pseudorapidity and boson rapidity distributions with
possibility to apply lepton \(p_{\perp}\) cuts.

The calculated leading order cross sections are multiplied by
NLO or NNLO K-factors provided for corresponding data distributions.
%%%%
\subsubsection{Next Leading Order}
%%%%%%%%%%%
\subsection{$t\bar{t}$ Cross Sections via {\tt HATHOR}}
Top-quark pairs ($t\bar{t}$) are mainly produced via $gg$ fusion and
$q \bar q$ annihilation. Furthermore, there are the $q q'$ and
$q g$ production modes.
The program HATHOR~\cite{Aliev:2010zk} allows calculating
the expected total $t \bar t$ cross section at hadron colliders
($p \bar p$ and $p p$) up to approximate NNLO accuracy.
Version 1.3 of HATHOR includes the exact NNLO for $q \bar q \to t \bar t$ \cite{Baernreuther:2012ws}
as well as a new high-energy constraint on the approximate NNLO obtained from
soft-gluon resummation \cite{Moch:2012mk}.
The default choice for renormalization and factorization scale in $t \bar t$ production is the top-quark mass, $m_t$.
The pole mass scheme is typically employed for $m_t$ but HATHOR also supports calculations in
the $\overline{\text{MS}}$ scheme.
\subsection{Jets}
The calculation of higher order jet cross sections is very demanding
in means of computing power. The reasons are the large number of contributing
Feynman diagrams and also the large number of infrared divergencies.
For an accurate cancellation of these singularities, typically, the 
dipole subtraction method is applied in such calculations.
During the necessary Monte Carlo integration a very fine phase
space sampling has to be performed in order to account for the
accurate cancellation of the counter terms.

In order to enable the inclusion of jet-cross section 
measurements in PDF and $\alpha_s$ fits, these perturbative
coefficients have to be pre-computed in a PDF and $\alpha_s$ 
independent way. For this purpose, two quite similar tools are
interfaced to the HERAFitter.

\subsubsection{FastNLO}
The fastNLO project~\cite{Kluge:2006xs,Wobisch:2011ij,Britzger:2012bs}
enables the inclusion of jet data in PDF and $\alpha_s$ fits.
This tool uses multi-dimensional interpolation
techniques to convert the convolutions of perturbative 
coeffcients with parton distribution functions and 
the strong coupling into simple products.
Although the concept is process independent, the perturbative 
coefficients are usually calculated by the \texttt{NLOJET++}
program~\cite{Nagy:1998bb} where calculations for jet-production
in DIS~\cite{Nagy:2001xb}  as well as in hadron-hadron 
collisions~\cite{Nagy:2003tz,Nagy:2001fj} are available.
Also threshold-corrections of $\mathcal{O}$(NNLO) for 
inclusive jet cross sections in hadron-hadron collisions are
available~\cite{Kidonakis:2000gi}.

The fastNLO libraries are standardized included in the HERAFitter
package and no further requirements or compilation options
are needed. In order to include a new measurement into the PDF-fit,
the fastNLO table have to be specified. These tables include all
necessary informations of the perturbative coefficients and the
calculated process for all bins of a certain dataset. 
Tables for almost all published jet measurements
are available through the project website {\tt http://fastnlo.hepforge.org},
or have otherwise to be calculated by using the full fastNLO package.

Features of the fastNLO concept are the very quick convolution of the
perturbative coefficients with the PDFs of
$\mathcal{O}(100 ms)$ and the very high accuracy
of the interpolation procedure. 
The fastNLO tables are conventionally calculated
for multiple factors of the factorization scale, 
and the renormalization scale factor can be choosen freely.
Some of the fastNLO tables already involve a scale-independent
concept~\cite{Britzger:2012bs}, which allows for 
the free choice of the renormalization and the factorization
scale as a function of two pre-defined observables.
The evaluation of the strong coupling constant, which enters
the cross section calculation, is taken consistently from the 
QCDNUM evolution code.


\subsubsection{APPLGRID}
The APPLGRID~\cite{Carli:2010rw} package allows to compute a fast estimate
of NLO cross section for particular processes for arbitrary set of 
proton parton density functions. The package implements
calculation of cross section of electroweak boson (\(Z,W\))
production as well as jet production in proton-(anti)proton
collisions and DIS processes. 

The approach is based on storing perturbative coefficients
of NLO QCD calculations of final-state observables measured
in hadron colliders in look-up tables. The PDFs and the 
strong couplings are included during the final calculations,
e.g. during the PDF fits procedure. The method allows 
variation of factorization and renormalization scales in
calculations.

The look-up tables (grids) can be generated with modified versions of
of MCFM~\cite{Campbell:1999ah,Campbell:2010ff} or 
NLOjet++~\cite{Nagy:2001fj} software distributed
with the full version of APPLGRID package. NLO calculations
for the current analysis are performed with the help of APPLGRID
generated grids based on MCFM calculations. 

Run parameters
and electroweak parameters are set in MCFM in a standard way
via the input file and user's part of the code. 
Binning and definitions of the observables for which the
differential cross sections are needed are set in the 
APPLGRID code. 
The grid parameters \(x_1, x_2\) and \(Q^2\) binning
and interpolation orders are also defined in the code.

APPLGRID performs construction of the look-up tables in two 
steps: {\it (i)} exploration of the phase space in order
to optimize the memory storage and {\it (ii)} actual grid
construction in the phase space corresponding to the 
requested observables.

Afterwards the NLO cross sections are restored from the grids
with providing PDFs, \(\alpha_S\), factorization and 
renormalization scales and with QCD NNLO k-factors applied
if stated.

%%%%%%%%%%%
\subsection{DIPOLE models}

At low $x$ and low $Q^{2}$, virtual photon-proton scattering is described using the colour
dipole model formalism~\cite{NNZ:91}. Within this formalism, the scattering process is calculated as a fluctuation of the
photon into a quark-antiquark pair (dipole), with a lifetime $\propto\enskip 1/x$, which interacts with the proton.

Several approaches have been developed to phenomenologically describe the dipole-proton interaction
cross section, three of which are implemented in the HERAFitter. These are
the original model version (GBW)~\cite{Golec-Biernat:1998js}, a model based on the colour glass condensate approach
to the high parton density regime (IIM)~\cite{Iancu:2003ge}, and a modified GBW model by adding effects of the 
DGLAP evolution (BGK)~\cite{Bartels:2002cj}.
%%%%
\subsubsection{GBW model}
In the GBW model the dipole-proton cross section $\sigma_{\text{dip}}$ is given by
\begin{equation}
\label{eGBW}
   \sigma_{\text{dip}}(x,r^{2}) = \sigma_{0} \left(1 - \exp \left[-\frac{r^{2}}{4R_{0}^{2}(x)} \right]\right),
\end{equation}
where $r$ corresponds to the transverse separation between the quark and the antiquark, and $R_{0}^{2}$ is 
an $x$ dependent scale parameter, having the form $R_{0}^{2}(x)=\left(x/x_{0}\right)^{\lambda}$.
The free fitted parameters are the cross-section normalisation $\sigma_{0}$ as well as $x_{0}$ and $\lambda$.
%%%%
\subsubsection{IIM model}
The IIM model assumes an improved expression for dipole cross section which is based on the 
Balitsky-Kovchegov equation~\cite{Balitsky:1995ub}. The explicit formula for $\sigma_{\text{dip}}$ 
can be find in~\cite{Iancu:2003ge}. The free fitted parameters are the alternative scale parameter $\tilde{R}$, $x_{0}$ and $\lambda$.
%%%%
\subsubsection{BGK model}
The BGK model modifies the equation (\ref{eGBW}) by incorporating the LO and NLO DGLAP evolution
of the gluon distribution. This leads to the expression for the dipole cross section
\begin{equation*}
\label{eBGK}
   \sigma_{\text{dip}}(x,r^{2}) = \sigma_{0} \left(1 - \exp \left[-\frac{\pi^{2} r^{2} \alpha_{s}(\mu^{2}) xg(x,\mu^{2})}{3 \sigma_{0}} \right]\right).
\end{equation*}
The factorization scale $\mu^{2}$ has the form $\mu^{2} = C_{bgk}/r^{2}+\mu^{2}_{0}$.
This model uses the following gluon density at the starting scale $Q_{0}^{2}=1\mbox{ GeV}^{2}$
\begin{equation*}
\label{eqTH730}
   xg(x,Q^{2}_{0}) = A_{g} x^{-\lambda_{g}}(1-x)^{C_{g}}.
\end{equation*}
The free fitted parameters for this model are $\sigma_{0}$, $\mu^{2}_{0}$ and 3 parameters for gluon $A_{g}$, $\lambda_{g}$, $C_{g}$. The parameter $C_{bgk}$ is kept fixed: $C_{bgk} = 4.0$. 
%%%%
\subsubsection{Mixed with DGLAP model}
%%%%%%%%%%%
\subsection{Unintegrated PDFs using CASCADE}
%%%%%%%%%%%
\subsection{Diffractive DIS PDFs}
Diffractive DIS data are fitted within the 'proton vertex factorisation' approach where 
the diffractive DIS is mediated by the exchange of hard Pomeron and a secondary Reggeon.
The model supplied by the DiffDIS package provides values of the 'reduced cross section',
$\sigma_r = F_2 - y^2/(1+(1-y)^2) F_L$
which is expected to be the experimentally meausured quantity.
The model supplied by the DiffDIS package provides values of the 'reduced cross section',
$\sigma_r = F_2 - y^2/(1+(1-y)^2) F_L$
which is expected to be the experimentally meausured quantity.

%%%%%%%%%%%
\section{Reweighting Techniques}
%%%%
\subsubsection{Using Monte Carlo method as in NNPDF}
This module should be renamed as it is a bit misleading.
This modules refers to the reweighting technique developed
by the NNPDF collaboration in the context of the PDF determinations.
The NNPDF reweighting calculates the $\chi^2$ between a new data set and the old NNPDF replicas in order to determine which replicas are still able to describe the new data (they are kept) and which ones fail (they are thrown out).

The output of the procedure is a new, updated NNPDF set in LHAPDF format with a reduced number of replicas that describe the old and the new data well. 
Some additional check plots which give clues about the validity of the procedure for the given new data set are also provided.
%%%%
\subsubsection{Using Eigenvector reweighting as in MSTW}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PDF Parameterisation}
\label{sec:pdfparam}
%%%%%%%%%%%
\subsection{Standard Functional form}
%%%%
\subsubsection{CTEQ style}
%%%%
\subsubsection{HERAPDF style}
%%%%
\subsubsection{Flexible style}
%%%%%%%%%%%
\subsection{Chebyshev Polynomial}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\chi^2$ Definitions}
\label{sec:chi2}
%%%%%%%%%%%
\subsection{Using Nuisance Parameters}
%%%%
\subsubsection{Simple Form}

%%%%
\subsubsection{Scaled Form}

For a single data set, 
 the $\chi^2$ function can be defined as~\cite{H1:2009bp}
%
\begin{equation}
 \chi^2_{\rm exp}\left(\boldsymbol{m},\boldsymbol{b}\right) = %\\
%~~~=
 \sum_i
 \frac{\left[m^i
- \sum_j \gamma^i_j m^i b_j  - {\mu^i} \right]^2}
{ \textstyle \delta^2_{i,{\rm stat}}\left(m^i -  \sum_j \gamma^i_j m^i b_j\right)+
\left(\delta_{i,{\rm uncor}}\,  m^i\right)^2}
 + \sum_j b^2_j.
\label{eq:ave}\end{equation}
%
Here ${\mu^i}$ is the  measured central value  at a point $i$ 
with  relative statistical $\delta_{i,stat}$ 
and relative uncorrelated systematic uncertainty $\delta_{i,unc}$.
Further, $\beta_j$ denotes a nuisance parameter for
 a correlated systematic error  source of type $j$ with an uncertainty
 while
$\gamma^i_j$ 
quantifies the sensitivity of the
measurement ${\mu^i}$ at the point $i$ to the systematic source $j$. 
The function $\chi^2_{\rm exp}$ depends on the set of
underlying physical quantities $m^i$ 
(denoted as the vector $\boldsymbol{m}$) and 
 the set of systematic uncertainties $b_j$ ($\boldsymbol{b}$).
This definition of the $\chi^2$ function takes into account that
systematic uncertainties are proportional to the central values 
(multiplicative errors), whereas the statistical errors scale 
with the square roots of the expected number of events. 
Other scaling properties for the statistical and uncorrelated
systematic uncertainties are available as described in appendix~\ref{sec:herafitter}.
%%%%
\subsubsection{Generalised Scaled Form}

%%%%%%%%%%%
\subsection{Using Covariance Matrix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Treatment of the Experimental Uncertainties}
%%%%%%%%%%%
\subsection{Hessian Method}

%%%%%%%%%%%
\subsection{Monte Carlo Method}
%%%%%%%%%%%
\subsection{Regularisation methods}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Program Manual}
\label{sec:man}
%%%%%%%%%%%
\subsection{Steering files}
    The software behaviour is controlled by two files with steering commands.
    These files have predefined names:
    \begin{itemize}
      \item {\tt steering.txt}  --   controls main "stable" (un-modified during 
                         minimisation) parameters. The file also contains
                         names of data files to be fitted to, definition 
                         of kinematic cuts                              
      \item {\tt minuit.in.txt}
                   --  controls minimisation parameters and minimisation 
                         strategy. Standard Minuit commands can be provided
                         in this file
      \item {\tt ewparam.txt}    --  controls electroweak parameters such
         as W and Z boson masses and CKM matrix parameters.
    \end{itemize}
%%%%%%%%%%%
\subsubsection{Options for DIS fits}
%%%%
\subsubsection{Options for Jets}
%%%%
\subsubsection{Options for Diffractive fits}
%%%%
\subsubsection{Options for DY fits}
%%%%%%%%%%%
\subsection{Data file format}
\label{sec:dataformat}
   Experimental data are provided by the standard {\tt ASCII} text files. The files
   contain a "header" which describes the data format and the "data" in terms
   of a 2-dimensional table. Each line of the data table corresponds to a
   data point, the meaning of the columns is specified in the file header.

   For example, a header for HERA-I combined H1-ZEUS data for e+p neutral 
   current scattering cross section is given in the file

\begin{verbatim}
       datafiles/H1ZEUS_NC_e-p_HERA1.0.dat
\end{verbatim}

   The format of the file follows standard "namelist" conventions. Comments 
   start with exclamation mark.  Pre-defined variables are:
\begin{itemize}
     \item{\tt Name}        --- (string) provides a name of the data set
    \item{\tt  Reaction}    --- (string) reaction type of the data set. Reaction type is used 
                      to trigger corresponding theory calculation. The following 
                      reaction types  are currently supported by the HERAFitter:
                      \begin{itemize}
                        \item {\tt 'NC e+-p'}  -- double differential NC ep scattering
                                      (ZMVFS and RT-VFS schemes) 
                        \item {\tt 'CC e+-p'}  -- double differential CC ep scattering
                                      (ZMVFS scheme)
                        \item {\tt 'CC pp'}    -- single differential $d \sigma_{W^{\pm}}/d eta_{\ell^{\pm}}$
                                      production and W asymmetry at $pp$ and $p\bar{p}$ 
                                      colliders (LO+kfactors and APPLGRID interface)
                        \item {\tt 'NC pp'}    -- single differential $d \sigma_Z / d y_Z$ at $pp$ and
                                      $p\bar{p}$ colliders
                                      ({\tt LO} with k-factors and {\tt APPLGRID} interface)

                        \item 'pp jets APPLGRID' -- $pp\to$ inclusive jet production, using
                                     {\tt APPLGRID}
                      \end{itemize}                       
      \item {\tt NData}       --- (integer) specifies number of data points in the file. 
                     This corresponds to the number of table rows which 
                     follow after the header.
      \item {\tt NColumn}     --- (integer) number of columns in the data table.
      \item {\tt ColumnType}  --- (array of strings)
                      Defines layout of the data table. The following column types
                      are pre-defined: 'Bin', 'Sigma', 'Error' and 'Dummy'
                      The keywords are case sensitive. 'Bin' correspond to an
                      abstract bin definition, 'Sigma' corresponds to the data
                      measurement, 'Error' - to various type of uncertainties and
                      'Dummy' indicates that the column should be ignored.
      \item {\tt ColumnName}  --- (array of strings)
                      Defines names of the columns. The meaning of the name depends
                      on the ColumnType. For ColumnType 'Bin', ColumnName gives a
                      name of the abstract bin. The abstract bins can contain
                      any variable names, but some of them must be present for 
                      correct cross section calculation. For example, 'x', 'Q2' and
                      'y' are required for DIS NC cross-section calculation.
 
                      For ColumnType 'Sigma', ColumnName provides a label for 
                      the observable, which can be any string.
 
                      For ColumnType 'Error', the following names have special meaning:
                      \begin{itemize}
                       \item 'stat'  -- specifies column with statistical uncertainties;
                       \item 'uncor' -- specifies column with uncorrelated uncertainties;  
                       \item 'total' -- specifies column with total uncertainties. 
                                  Total uncertainties are not used in the fit,
                                  however there is an additional check is performed
                                  if 'total' column is specified: sum in quadrature
                                  of statistical, uncorrelated and correlated 
                                  systematic uncertainties is compared to the total
                                  and a warning is issued if they differ significantly.
                       \item'ignore' - specifies column to be ignored (for special studies).
                       \item Other names specifies columns of correlated systematic 
                      uncertainty. For a given data file, each column of the correlated
                      uncertainty must have unique name. To specify correlation across
                      data files, same name must be used for different files.  
                      \end{itemize}
      \item {\tt SystScales}  --- (array of float)
                      For special studies, systematic uncertainties can be scaled
                      The numbering of uncertainties starts from the first column
                      with the ColumnType 'Error'. For example, setting 
\begin{verbatim}
                  SystScale(1) = 2.  
\end{verbatim}
                      in {\tt datafiles/H1ZEUS\_NC\_e-p\_HERA1.0.dat} would scale stat. 
                      uncertainty by factor of two.                       
      \item {\tt Percent}     --- (array of bool) For each uncertainty specify if it is given in 
                      absolute ("false") or in percent ("true").  The numbering of 
                      uncertainties starts from the first column with the 
                      {\tt ColumnType} 'Error' (see example above).
      \item {\tt NInfo}       --- (integer) Calculation of the cross-section predictions may 
                      require  additional information about the data set. The number of 
                      information strings is given by NInfo
      \item {\tt CInfo}       --- (array of strings) Names of the information strings. 
                      Several of them are predefined for different cross-section 
                      calculations.
      \item {\tt DataInfo}    --- (array of float) Values, corresponding to {\tt CInfo} names.
      \item {\tt IndexDataset} -- (integer) Internal H1 Fitter index of the data set. Provide unique
                      numbers to get extra info for $\chi^2/dof$ for each data set.      
      \item {\tt TheoryInfoFile} --- (string) Optional additional theory file with extra 
                     information for cross-section calculation. This could be k-factors,
                     {\tt APPLGRID} file or {\tt FastNLO} table.  
      \item {\tt TheoryType} --- (string) Theory file type ('kfactor', 'applgrid' or 'fastnlo').      
      \item {\tt NKFactor}   --- (integer) For kfactor files, number of columns in
                     {\tt TheoryInfoFile}.
      \item {\tt KFactorNames} --- (array of strings) For kfactor files, names of columns in 
                     {\tt TheoryInfoFile}.
\end{itemize}

%%%%%%%%%%%
\subsection{Selection of the data}
  The namelist \&Cuts, located inside the {\tt steering.txt} file can be used to apply
  simple process dependent cuts. The cuts are limitted to bin variables.
  Simple low and high limits are allowed. For example, a cut on $Q^2>3.5$~GeV$^2$ for
  NC ep scattering is specified as

\begin{verbatim}
  ! Rule #1: Q2 cuts
   ProcessName(1)     = 'NC e+-p'
   Variable(1)        = 'Q2'
   CutValueMin(1)     = 3.5 
   CutValueMax(1)     = 1000000.0
\end{verbatim}

  Maximum 100 cuts can be used by default.
%%%%%%%%%%%
\subsection{Understanding the output}
  The results of the minimization are printed to the standard output and written
  to the files in the {\tt output/} directory. 

  The quality of the fit can be judged based on total $chi^2$ per degrees of freedom.
  It is printed for each iteration as 
\begin{verbatim}
                      Iteration   Chi2   NDF       Chi2/NDF
   FitPDF f,ndf,f/ndf      3      588.64 579        1.02
\end{verbatim}
  The resulting $chi^2$ is reported at the end of minimisation for each data set and for correlated 
  systematic uncertainties separately. This information is printed and written
  to the {\tt output/Results.txt} file. The {\tt Results.txt} file contains additional 
  information about shifts of the correlated systematic uncertainties.

  The minimization information from the {\tt minuit} program is stored using the standard {\tt minuit} in the {\tt output/minuit.out.txt}
  file. The level of verbosity for this information can be changed by {\tt minuit} commands
  in the {\tt minuit.in.txt} file. Make sure that {\tt minuit} does not report any errors
  or warnings at the end of minimisation.
  
  Point by point comparison of the data and predictions after the minimization 
  is provided in {\tt output/fittedresults.txt} file. The file reports three columns
  corresponding to the three first bins of the input tables, data value, sum in 
  quadrature of statistical and uncorrelated systematic uncertainty, total
  uncertainty, the predicted value, before and after applying correlated systematic shifts,
  pull betweenthe  data and theory and 
  data set index. The pull $p$ is calculated as 
  \begin{equation}
      p = \frac{ \mu - m} {\sigma_{\rm uncor}}
  \end{equation}
  where $\mu$ is the data value, $m$ is the prediction and $\sigma_{\rm uncor}$ is the total
  uncorrelated uncertainty.
  Similar information is stored in the {\tt pulls.first.txt} and {\tt pulls.last.txt} files
  ( dataset index, first bin, second bin, third bin, theory, data, pull).
  Theory is  adjusted for systematic error shifts in this case.

  The output PDFs are stored in  {\tt output/pdfs\_q2val\_XX.txt} files.
  Each of the files reports values of gluon, and quark PDFs as a function of $x$
  for fixed $Q^2$ points. The $Q^2$ values and $x$ grid are specified by 
  {\tt \&Output} namelist in the {\tt steering.txt} file.
  
  The PDF information and data to theory comparisons can be plotted using 
  the {\tt bin/DrawResults} program.  Calling it without arguments plots results from
  {\tt output/} directory. Given the program one argument specifies sub-directory 
  where the information is read. Calling the {\tt bin/DrawResults} program with two
  arguments provides comparison of the PDFs obtained in the two fits.
  
  Finally, the \fitter\ package provides PDFs in the {\tt LHAPDF} format. To obtain the
  {\tt LHAPDF} grid file, run the {\tt tools/tolhapdf.cmd} script. The script produces 
  the {\tt PDFs.LHgrid} file which can be read by the lhapdf version lhapdf-5.8.6.tar.gz
  or later.
%%%%%%%%%%%
\subsection{{\tt Minuit} steering cards}

%%%%%%%%%%%%%%%%%%%%%%%%%
\section{User Examples}
%%%%
\subsection{DIS inclusive only}

%%%%
\subsection{All processes}


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{writeup.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{{\tt \&HERAFitter} namelist format}
\label{sec:herafitter}
\begin{itemize}
  \item {\tt ITheory} --- (integer) Currently only QCDNUM standard evolution
     is implemented for which {\tt ITheory} is set to 0.
  \item {\tt IOrder} --- (integer) For {\tt ITheory} =0 (collinear factorisation) : 
        LO fit (1) or NLO (2) or NNLO (3) 
  \item {\tt Q02} --- (float) Evolution starting scale.
  \item {\tt HF\_SCHEME} --- Specify heavy quark flavour treatment for neutral
 current $ep$ process. The following schemes are implemented: 
    \begin{itemize}
      \item {\tt 'ZMVFNS'}: Zero Mass Variable Flavour Number Scheme, as implemented
 in {\tt QCDNUM}.
      \item {\tt 'RT'}: Thorne-Roberts VFN scheme for $F_2^{\gamma}$. 
      \item {\tt 'RT FAST'}: Fast approximate RT VFN scheme using k-factor 
with respect ot QCDNUM ZMVFNS, calculated at the first iteration.
    \end{itemize}
\item {\tt PDFStyle} --- (string) PDF parameterisation style. Possible styles are currently available:
   \begin{itemize}
  \item{\tt '10p HERAPDF'} -- HERAPDF-like with an extra assumption 
                                 $B_{u_v} = B_{d_v}$;
  \item{\tt '13p HERAPDF'} -- HERAPDF-like with $B_{u_v}$ and $B_{d_v}$ 
                          floated independently;
  \item{\tt '10p H12000'}  -- H12000-like with independent PDFs being the
               $D,U,\bar{D},\bar{U}$ quarks and gluon.
  \item{\tt 'CTEQ'}        -- CTEQ-like parameterisation.
  \item{\tt 'CHEB'}        -- CHEBYSHEV parameterisation based on 
         gluon,sea, $u_{v}$, $d_{v}$ independent pdfs.
 \end{itemize}
\item {\tt CHI2Style}  --- (string) choice of the $\chi^2$ function:
   \begin{itemize}
   \item {\tt 'H12000'} -- Pascaud-like, systematic shifts to theory, no scaling of statistical, uncorrelated errors.
   \item {\tt 'HERAPDF'} -- Pascaud-like + "mixed error scaling"
   \item {\tt 'HERAPDF Sqrt'}   -- Pascaud-like + "sqrt error scaling"
   \item {\tt 'HERAPDF Linear'} -- Pascaud-like + "linear error scaling"
 \end{itemize}
  \item {\tt LDEBUG}  --- (logical) debug flag.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
\section{How do add new module}
\subsection{Structure of the module}
\subsection{Theory Interface Example}
\subsection{Steerings and Configurations}
%%%%%%%%%%%%%%%%%%%%%
\section{How to add new data}
Inclusion of the data files is controlled by {\tt \&InFiles} namelist in the 
{\tt steering.txt} file. For example, by default the following four HERA-I
    files are included:
\begin{verbatim}
&InFiles
    NInputFiles = 4
    InputFileNames(1) = 'datafiles/H1ZEUS_NC_e-p_HERA1.0.dat'
    InputFileNames(2) = 'datafiles/H1ZEUS_NC_e+p_HERA1.0.dat'
    InputFileNames(3) = 'datafiles/H1ZEUS_CC_e-p_HERA1.0.dat'
    InputFileNames(4) = 'datafiles/H1ZEUS_CC_e+p_HERA1.0.dat'
&End
\end{verbatim}

To include more files:
\begin{itemize}
 \item  Increase the {\tt NInputFiles} variable.
 \item  Specify the additional file by providing corresponding
  {\tt InputFileNames()} variable.
\end{itemize}
Details about data file format can be found in section~\ref{sec:dataformat}.



\end{document}

