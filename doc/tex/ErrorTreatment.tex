%%%%%%%%%%%
\subsection{Hessian Method}

%%%%%%%%%%%
\subsection{Offset Method}

\newcommand{\rs}{s}
\newcommand{\ce}{b}
\DeclareRobustCommand\Vstat{\ensuremath{V^{\mathrm{(unc)}}}\ssp}
\DeclareRobustCommand\Vsys{\ensuremath{V^{\mathrm{(cor)}}}\ssp}
\DeclareRobustCommand\mb[1]{\ensuremath{\mbox{\mathversion{bold}{$#1$}}}\ssp}
\DeclareRobustCommand\mbs[1]{\ensuremath{\mbox{\mathversion{bold}{\scriptsize $#1$}}}\ssp}

\subsubsection {Correlated errors and \texorpdfstring{$\chi^2$}{chi2}}
\label{sec:cor-chi2}

% \cite{Chekanov:2002pv,Pascaud:1995qs}
% A typical formula for $\chi^2$ depending on the correlated systematic errors' fluctuations reads
Results of a measurement can be modelled as
% (see \eg \cite{Stump:2001gu,Pascaud:1995qs})
(see \eg \cite{Stump:2001gu,Botje:2001fx})
\begin{equation}
m_n = t_n(a) + r_n \sigma_n + \sum_{\mu=1}^K \rs_\mu \ce_{n\mu}
\;,\quad n=1,\dots,N
\end{equation}
where\\
$m_n$ is the value measured for the $n$-th data point,\\
$t_n(\mb a)$ is true (theoretical) value depending on parameters $\mb a = (a_1,\dots, a_M)$,\\
$\sigma_n$ is the uncorrelated error,\\
$\ce_{n\mu}$ are the errors from the $\mu$-th correlated error source,\\
$r_n$ and $\rs_\mu$ are random variables fluctuating around 0 with unit dispersion.

First, we assume that all $r_n$ are uncorrelated with $\rs_\mu$,
mutually independent and normally distributed,
%  with the statistical errors.
\begin{equation}
\rho(r) = \frac{e^{-r^2/2}}{\sqrt{2\pi}}
\,.
\end{equation}

In the following we will use scaled variables
\begin{subequations}
\begin{eqnarray}
x_i &\equiv& \frac{m_i-t_i}{\sigma_i}
\,,
\\
\beta_{i\mu} &\equiv& \frac{\ce_{i\mu}}{\sigma_i}
\,.
\end{eqnarray}
\end{subequations}

Keeping $\mb \rs$ fixed we get the probability density of measurements,
\begin{equation}
dp(\mb{m}| \mb s) =
 (2\pi)^{-N/2}\, e^{-\chi_1^2(\mbs\rs)/2}\, d^Nx
\,,
\end{equation}
where
\begin{equation}
\label{eq:chi1}
\chi_1^2(\mb\rs) = \sum_{n=1}^N
\left( x_n - \sum_\mu \beta_{n\mu} \rs_\mu \right)^2
\equiv (\mb{x - \beta s})^2
\,,
\end{equation}

Further, taking into account the probability distribution of the correlated error sources,
$p(\mb s)\, d^Ks$, we have
\begin{equation}
p(\mb{m},\mb s) = p(\mb s)\, p(\mb{m} | \mb s)
\,.
\end{equation}
Assuming again the uncorrelated normal distribution,
\begin{equation}
p(\mb s) = \prod_{\mu=1}^K \frac{e^{-s_\mu^2/2}}{\sqrt{2\pi}}
\,,
\end{equation}
we get
\begin{equation}
\label{eq:p_ms}
dp(\mb{m},\mb s) =
  (2\pi)^{-(N+K)/2}\, e^{-\chi_{\mathrm c}^2(\mbs\rs)/2}
  \,d^Nx\, d^Ks
\,,
\end{equation}
with
\begin{equation}
\label{eq:chi_c}
\chi_{\mathrm c}^2(\mb\rs) = 
% (\mb{x - \beta s})^{\mathrm{T}} (\mb{x - \beta s}) + \mb s^{\mathrm{T}} \mb s
(\mb{x - \beta s})^2 + \mb s^2
\,.
\end{equation}

This quadratic form in $\mb s$ allows for analytical integration of \Eq{eq:p_ms}
resulting in

\begin{equation}
p(\mb{m}) \propto e^{-\chi^2/2}
\,,
\end{equation}
where
\begin{equation}
\label{eq:chi_int}
\chi^2 = \mb x^{\mathrm{T}} \mb{A\,x}
\end{equation}
% with constant $\mb A$ determined by $\mb\beta$ and $\mb\sigma$
with $\mb A$ depending on $\mb\beta$ only
(see \eg \cite{Stump:2001gu} Appendix B).

% If we assume that $\rs_\mu$ are random variables with normal distribution, we have
% \begin{equation}
% \tilde\chi^2(\rs) = \sum_{\mu=1}^K \rs_\mu^2
% \end{equation}
% and we can integrate them out analytically.
This is \eg the CTEQ approach described in \cite{Stump:2001gu}.
It is worth noting that the solution \Eq{eq:chi_int} for $\chi^2$
can be obtained by minimizing $\chi_{\mathrm c}^2(\mb\rs)$ of \Eq{eq:chi_c} wrt. $\mb\rs$.

% -----------------------------------
\subsubsection{The Offset method}

In the Offset method presented here we assume that $\mb\rs$ is fixed, 
and we find the best theoretical model by minimizing
$\chi_1^2$ wrt. to $\mb a$. 
Hence the fitted parameters become functions
of $\mb\rs$. %, $\mb a = \mb a(\mb\rs)$. 
We do not impose any particular statistical properties on $\mb\rs$ 
and we take $\mb a(\mb\rs=0)$ as the ultimate fit result for the theory parameters. 
The dependence on $\mb\rs$ is, however, used to determine
the full error matrix of $\mb a$ (\cf \cite{Pascaud:1995qs}).

The full covariance matrix $V$ reads
\begin{equation}
\label{eq:Cv-full}
V = \Vstat + \Vsys
\end{equation}
% with $\Vsys$ coming from the $\rs$ fluctuations.
For each $\mb\rs$ we find the parameters $\mb a(\mb \rs)$ by minimising $\chi_1^2(\mb \rs)$, which results in
% $a = a(\rs)$ and 
$\Vstat(\mb \rs) = M^{-1}(\mb \rs)$ where
\begin{equation}
M_{jk}(\mb \rs)
= \left.
{\frac12} \frac{\partial^2\chi_1(\mb \rs)^2}{\partial a_j \partial a_k}\right\vert_{\mb a = \mb a(\mb \rs)}
\,.
\end{equation}
The dependence of $M$ on $\mb \rs$ is considered to be a higher order correction
and we take $\Vstat = M^{-1}(0)$.
% Nb. $\Vstat$ is the covariance matrix returned by Minuit for the fit with $\mb \rs =0$.

Within linear approximation to the error propagation
\begin{equation}
\label{eq:Vsysp}
\Vsys_{jk} = \sum_\mu \frac{da_j}{d\rs_\mu} \frac{da_k}{d\rs_\mu}
% \,.
\end{equation} 
and we calculate the derivatives as
\begin{equation}
\frac{da_j}{d\rs_\mu} \approx
\frac{a_j(\rs_\mu=\epsilon) - a_j(\rs_\mu=-\epsilon)}{2\epsilon}
% \,,
\end{equation}
with $\mb a(\rs_\mu=\epsilon)$ resulting from
fits to the data shifted by $\epsilon\ce_{n\mu}$.

In the code we use $\epsilon=1$, \ie one standard deviation of
the correlated error source which, in the ideal statistical limit, corresponds to 
$\Delta\chi_1^2 = 1$.
On the other hand, within the leading approximation, the value of $\epsilon$ is irrelevant.
% in accordance to the standard Minuit normalization of $\Vstat$.

If another error definition, $\Delta\chi_1^2 = \lambda$, 
is adopted\footnote{E.g. Jon Pumplin uses $\lambda=5$, \cf \texttt{minuit/iterate.F}} then
the full covariance matrix, $V$, must be scaled by $\lambda$. 

%==========================================
\subsubsection {Offset method user's guide}


% -----------------------------------------
{\bf {Correlated error sources}}

The correlated systematic error sources (CSS) are identified by \fitter\ upon reading data files.
Internally, the actual number of CSS 
is stored in the variable \verb'NSYS'
and the maximum allowed number of CSS = \verb'NSYSMAX' (currently 300).
\vspace{0.4cm}

% -----------------------------------------
{\bf {PDFs errors bands}}

The errors bands for the fitted PDFs are calculated 
basing on the full covariance matrix $V = \Vstat + \Vsys$,
and are saved to the 'standard' output files
\verb'pdfs_q2val_*.txt' and \verb'pdfs_*.lhgrid'.
\vspace{0.4cm}

% -----------------------------------------
{\bf {Input parameters}}

File: \verb'steering.txt'

The Offset method is turned on by setting\\
\verb:CHI2Style = 'Offset':

By default all fits are run in a single job, each fit driven by initial parameters and Minuit commands
read from \verb'minuit.in.txt'.

Two optional parameters can be set in the \verb'CSOffset' NAMELIST, \eg
\vspace*{-2.5ex}
\begin{verbatim}
&CSOffset
  CorSysIndex  =  0
  UsePrevFit = 1
&End
\end{verbatim}
\vspace*{-1ex}
Defaults are set in \verb'read_steer.f'
and the \verb'CSOffset NAMELIST' is read only when the Offset method is active.
\vspace{0.4cm}

% ................................
{\tt CorSysIndex}

Default: \verb'CorSysIndex = NSYSMAX+1'

Setting \verb'CorSysIndex' to any value $\in [-K, K]$ 
restricts the job to a single fit to data shifted (down or up) by a corresponding
correlated error source.
\verb'CorSysIndex' = 0 corresponds to the central fit.

If \verb'CorSysIndex > NSYSMAX' then all the fits are performed.
The best way to perform all fits in a single run is 
to not specify \verb'CorSysIndex' at all.
\vspace{0.4cm}

% ...............................
{\tt UsePrevFit}

This parameter determines how to use results of previous fits,
if such results are present in the \verb'output' folder.

Default: \verb'UsePrevFit' = 0

\begin{enumerate}
\item [0 ---]
Do not use any previous fit results
\item [1 ---]
Use previously obtained parameters as starting values for the current fit.
% For a non-offset fit read initial parameters from \verb'minuit.save.txt';
% for an offset fit 
Read initial parameters from \verb'minuit.save_<CSI>.txt'
 --- \eg  \verb'minuit.save_001m.txt' for \verb'CorSysIndex' $= -1$.
If the file does not exist and \verb'CorSysIndex' $\neq 0$ try to read
\verb'minuit.save_0.txt'.
\item [2 ---]
Do not perform the fit if a corresponding \verb'Results_<CSI>.txt' file exists,
otherwise switch to mode 1.
\end{enumerate}
\vspace{0.4cm}

% -----------------------------------------
{\bf {Running the fits}}

First, the central fit (\verb'CorSysIndex' = 0) should be thoroughly performed,
resulting in the \verb'minuit.save_0.txt' file with final parameter values.
Next the fits for \verb'CorSysIndex' $\neq 0$
can be run. Setting \verb'UsePrevFit' > 0 will set the initial parameters' values to the `central' ones,
which will speed up the minimisations for non-central fits
and will help to avoid landing in a false minimum.
Eg. setting \verb'UsePrevFit' = 2 and not specifying \verb'CorSysIndex' 
will run all missing fits in one job;
you could run \verb'JayRun.tcl -use 2' (see below).

% Nb. for a job performing all the fits in a single run (\verb'CorSysIndex' unspecified),
% the central fit is done first.

For many CSS one should consider running jobs on some batch system (a `farm').
To this end the code is organised in such a way that in the Offset mode the results of each fit are saved to files
(in the \verb'output' folder). For the central fit the parameters are saved to
\verb'params_0.txt'
and the covariance matrix, $\Vstat$, to \verb'statcov_0.txt';
for other fits, the parameters are saved to
\verb'params_'$j$\verb'{m|p}.txt', for $j = 1,\dots,K$.
Once all results are known the full covariance matrix 
% (Eq. \Eq{eq:Cv-full}) 
is calculated and saved to \verb'offset.save.txt'.
Error bands are calculated if requested in the steering cards.

Actually, each run of the \fitter\ ends with an attempt to perform this final calculation.
If any of the required files is missing the covariance matrix is not calculated
and a warning message is printed.
This behaviour allows for an easy and flexible arrangement of a script-driven parallel computation.
% Running a fit for any {\tt CorSysIndex} always ends with an attempt to collect the final results --- 
% and it succeeds once all results are known.

% We recommend the following sequence:
A typical scenario is:
\begin{enumerate}
\item
run the central fit (\verb'CorSysIndex' = 0),
% --- this makes \verb'minuit.save_0.txt' file with final parameter values,
\item
run fits for \verb'CorSysIndex' $\neq 0$;\\
set \verb'UsePrevFit' = 1
and place \verb'minuit.save_0.txt' in the \verb'output' folder before running the fit,
% --- this will set the initial parameters' values to the `central' ones,
\item
collect all the results in the \verb'output' folder and run the final job with
\verb'UsePrevFit' = 2.
\end{enumerate}

\goodbreak
% ...............................
{\bf {Scripts for batch running}}

A set of example scripts
which facilitate running jobs on a farm is supplied in the \verb'tools/RunJobs' folder.
Configurations for the NQS and ZEUS ZARAH farms are provided.
At the end of this section we give hints on how to customize the scripts for another environment.

\newcommand\HF{\textit{HF}}
In the following
`\HF' denotes your \fitter\ installation folder
and the paths below are relative to this folder.

The scripts must be run from the folder where you run \texttt{FitPDF} locally. 
In other words, the folder must contain input files:\\
\texttt{ewparam.txt},
\texttt{minuit.in.txt},
\texttt{steering.txt},\\
and moreover 
\texttt{datafiles} must be seen as a subfolder. 
Thus \HF\ can be your `run' folder, although I recommend creating a separate folder 
outside the \fitter\ structure, and making there the link\\
\texttt{ln -s \HF/datafiles datafiles}
% \texttt{ln -s \HF/bin bin}

There are three production scripts:
\begin{itemize}
\item \texttt{JayRun.tcl}\\
Runs fit locally for default values of the Offset parameters
(no specific \verb'CorSysIndex' and \verb'UsePrev=0');\\
\verb'JayRun.tcl [-ind <CorSysIndex>] [-use <UsePrev>]'\\
runs according to parameters given.

\item \texttt{JaySub.tcl}\\
Submit all non-central jobs.
The submission will fail if the \verb'output' folder
does not contain results from the central fit.
Identifiers of submitted jobs are written to `JobLog' (\verb'job_work/Last.jobs' file).

\item \texttt{JayGet.tcl}\\
Retrieve jobs. Only jobs from JobLog are looked for.
The number of still running jobs is displayed. 
Each retrieved job is removed from JobLog.
%  job_work/Last.jobs is deleted once there no more running jobs.
There are two useful options, \texttt{peek} and \texttt{run}, \eg\\
\verb'JayGet.tcl -peek 100 &'\\
runs until all jobs are completed, peeking every 100 seconds and retrieving completed jobs.
It asks for starting the final run once all jobs are done.
If you answer `no', you can later run \verb'JayRun.tcl -ind 0 -use 2'.
This final run can be automated using option \texttt{run}, \eg\\
\verb'JayGet.tcl -p 300 -run &'\\
which peeks every 300 seconds and performs the final run once all jobs are retrieved.
\end{itemize}

\goodbreak
The configuration options are given in \verb'jay.cfg.tcl'.
This file is read from the folder where the currently called script resides
(\verb'tools/RunJobs' by default).
If a file named \verb'jay.cfg.tcl' exists in the current folder it is read next
--- this provides a method to modify settings locally.

The most important parameters in \verb'jay.cfg.tcl':
\begin{itemize}
\item
\verb'FarmName' --- default = \verb'NQS'\\
The farm specific commands are read from the \texttt{\textit{FarmName}.tcl} file.\\
We supply \verb'NQS.tcl' and \verb'zarah.tcl' containing
routines specific to the corresponding farms.
Note that the ZARAH farm requires a one-time authentication via \verb'zarah-auth',
before starting any activity.

\item
\verb'FarmHost' --- default = "" (empty)\\
The NQS farm host name.

\item
\verb'FarmUser' --- default = "" (empty)\\
The NQS farm user name.

\item
\verb'Driver' --- default = \verb'jay.sh'\\
This `driving script' is sent to the farm as the primary code to execute.
\verb'Driver' is first searched for in the current folder
and next in the folder of the currently called script.
Hence, even without changing this parameter, you can have your own version of the 
driving script (\verb'jay.sh') saved in your current `run' folder.

\item
\verb'MainExe' --- default = \verb'bin/FitPDF'\\
The path to the \fitter\ executable.

\end{itemize}

The two remaining files, \verb'utils.tcl' and \verb'job_farm_fns.tcl', define various utility routines.
They contain no `user serviceable' parts and are independent of a particular farm operation.

In order to add new functionality for some \verb'NewFarm' it should be sufficient to
create a \verb'NewFarm.tcl' file analogous to the supplied
\verb'NQS.tcl' or \verb'zarah.tcl' files, and set proper values of
\verb'FarmName' etc. in your \verb'jay.cfg.tcl' file.

\goodbreak
% ...............................
{\bf {Examples}}

Typically you would proceed as follows:
\begin{enumerate}
\item
run the central fit\\
\HF\verb'/tools/RunJobs/JayRun.tcl -ind 0'\\
or\\
\HF\verb'/tools/RunJobs/JayRun.tcl -ind 0 -use 1'
\item
submit offset fits to the farm\\
\HF\verb'/tools/RunJobs/JaySub.tcl'
\item
collect results and calculate final results\\
\HF\verb'/tools/RunJobs/JayGet.tcl -peek 300 -run'
\end{enumerate}



%%%%%%%%%%%
\subsection{Monte Carlo Method}
\label{sec:ToyMC}

The PDF uncertainties can be estimated using a Monte Carlo technique \cite{mcmethod}.
The method consists in preparing replicas of data sets by allowing the central values of the cross sections to 
fluctuate within their systematic and statistical uncertainties taking into account all point-to-point correlations.
The preparation of the data is repeated for a large $N$ ($>100$ times) and for each of these replicas a NLO QCD fit is performed to 
extract the PDF set. The PDF central values and uncertainties are estimated using the means values and RMS 
over the replicas. 



%%%%%%%%%%%
\subsection{Regularisation methods}


