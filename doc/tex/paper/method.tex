
%\fitter\ is a unique QCD fit platform that provides many options to the user to perform a quantitative assessment of impact level for a new data or new theoretical prediction. The quest on nailing down the uncertainties on PDFs have lead, on one hand, to highly precise measurements that are in need of careful handling of all provided sources of uncertainties, and on the other hand to numerous software packages that provide higher order calculations in QCD to match the precision of data.


Performing a QCD analysis one usually needs to check stability of the results
w.r.t. different assumptions,
%There is a considerable number of choices available when performing a QCD analysis on
e.g. the functional parametrisation form, the heavy quarks mass values, alternative theoretical calculations,
method of minimisation, interpretation of uncertainties, etc.
%
 It is also desirable to be able to discriminate or quantify the effect of the chosen ansatz, ideally within a common framework, and 
\fitter is optimally designed for such tests.
%
The methodology employed by \fitter  relies on a flexible and modular
framework that allows for independent integration of the state-of-the-art techniques, either related to the inclusion of a new theoretical calculation, or of new approaches to treat uncertainties. 

In this section we briefly describe the available options in \fitter.
%ranging from the functional form used to parametrise PDFs and the choice of the form of the $\chi^2$ function, to different methods to assess the experimental uncertainties on extracted PDFs.
%
%list various types of functional forms used to parametrise PDFs, different definitions for  $\chi^2$ evaluation in extracting the PDF parameters which account for correlated and uncorrelated sources of experimental (or theoretical) uncertainties available in \fitter. 
In addition, as an alternative approach to a complete QCD fit, the Bayesian reweighting
method, which is also available in \fitter, is described.



\subsection{Functional Forms for PDF parametrisation}
The PDFs are parametrised at a starting scale, chosen to be below charm mass.
%by the set of default defined PDFs in \fitter.
In \fitter various functional forms to parametrise PDFs can be used:
%using free parameters to be extracted from the fit:
\begin{description}
\item \bf {Standard Polynomials:} \rm
%The term refers to using a simple polynomial to interpolate between the low and high $x$ regions:
A polynomial form is used to parametrise the $x$-dependence of the PDFs:
%The term standard is understood to refer to a simple polynomial 
%that interpolates between the low and high $x$ regions:
\begin{equation}
 xf(x) = A x^{B} (1-x)^{C} P_i(x),
\label{eqn:pdf_std}
\end{equation}
The standard polynomial form is most commonly used by the PDF groups.
In HERA PDFs, the parametrised PDFs are the valence distributions
$xu_v$ and $xd_v$, the gluon distribution $xg$, and the $u$-type and $d$-type sea 
$x\bar{U}$, $x\bar{D}$, where $x\bar{U} = x\bar{u}$, 
$x\bar{D} = x\bar{d} +x\bar{s}$ at the starting scale. 
%chosen below the charm mass threshold. 
The form of polynomials $P_i(x)$ depdend on the style, defined as a steering parameter. 
For the HERAPDF~\cite{h1zeus:2009wt} style takes the Regge-inspired form  
$(1 + \epsilon \sqrt{x} + D x + E x^2)$
with additional constraints relating to the flavour decomposition of the 
light sea. 
For the CTEQ style, $P_i(x)$ takes the form $e^{a_3x} (1 + e^{a_4} x + e^{a_5} x^2)$.
QCD number and momentum sum-rules are used to determine the normalisations $A$ for the valence and gluon
distributions, and the sum-rule integrals are soved analytically.

\item \bf {Bi-Log-Normal Distributions:} \rm
%A bi-log-normal distribution to parametrise the $x$ dependence of the PDFs is 
%also available in \fitter.
%parton distribution function of the proton.
The parametrisation is motivated by multi-particle statistics
%~\cite{hera-lhc:report2009}
and holds the following functional form:
%\begin{center}
\begin{equation}
 xf(x)=a x^{p-b\log(x)}(1-x)^{q-d \log(1-x)}.
\label{eqn:bilog}
\end{equation}
%\end{center}
This function can be regarded as a generalisation of the standard polynomial form described above,
however, numerical integration of Eq.~\ref{eqn:bilog} is required in order to satisfy the QCD sum rules.
%In order to satisfy the QCD sum rules this parametric form requires numerical integration.

\item \bf {Chebyshev Polynomials:} \rm
A flexible parameterization employed for the gluon and sea distributions
and based on the Chebyshev polynomials.
%A flexible Chebyshev polynomial based parametrisation can be used for the gluon and sea densities. 
For better modeling the low-$x$ asymptotic of those PDFs, the polynomial of the argument $\log(x)$ are considered.
%The polynomials use $\log x$ as an argument to emphasize the low $x$ behavior. 
Furthermore, the PDFs are multiplied
by the factor of $(1-x)$ to ensure that they vanish as $x\to 1$. The resulting parametric form reads

{ \small
\begin{eqnarray}
x g(x) &=& A_g \left(1-x\right) \sum_{i=0}^{N_g-1} A_{g_i} T_i \left(-\frac{\textstyle 2\log x - \log x_{\rm min} } {\textstyle \log x_{\rm min} } \right)\,, \label{eq:glu} \\
x S(x) &=& \left(1-x\right) \sum_{i=0}^{N_S-1} A_{S_i} T_i \left(-\frac{\textstyle 2\log x - \log x_{\rm min} } {\textstyle \log x_{\rm min} } \right)\,, \label{eq:sea} 
\end{eqnarray}
}
where $T_i$ are the first-type Chebyshev polynomials of the order $i$.
%The values of $N_{g,S}$ up to 15 are allowed, however, already
%starting from $N_{g,S}>5$ the fit quality is similar to the one based on
%the standard-polynomial parametrisation with a similar number of parameters.
The normalisation factor $A_g$ is defined from the momentum sum rule which
can be evaluated analytically.
The values of $N_{g,S}$ up to 15 are allowed, however, already starting from $N_{g,S} \ge 5$ 
the fit quality is already similar
to the standard-polynomial parametrisation with a similar number of parameters.
%
%where the sum runs over $i$ up to $N_{g,S}=15$ order Chebyshev polynomials of the first type $T_i$ for
%the gluon, $g$, and sea-quark, $S$, density, respectively. 
%The normalisation factor $A_g$ is defined from the momentum sum rule.
%%
%The advantages of this parametrisation are that the momentum sum rule can be evaluated analytically 
%and that for $N \ge 5$ the fit quality is already similar
%to the standard Regge-inspired parametrisation with a similar number of parameters.

%A study of the parametrisation uncertainty at low Bjorken $x \le 0.1$ for PDFs was presented 
%in~\cite{Chebyshev}. Figure~\ref{fig:cheb} shows the comparison of the 
%gluon density detemined from the HERA data  
%with the standard and the Chebyshev parametrisation.
%
The low-$x$ uncertainties in the PDFs determined from the HERA data using different 
parameterizatons were studied in~\cite{Chebyshev}. Figure~\ref{fig:cheb} shows the comparison 
of the gluon density obtained with the parameterization Eq.~\ref{eq:glu},\ref{eq:sea} to 
the standard-polynomial one.

%with a reduced parametrisation uncertainty. 
%An additional regularisation prior leads to a 
%significantly reduced uncertainty for $x \le 0.0005$.
\begin{figure}[!ht]
 \centering
  \includegraphics[width=5cm]{chebishev.pdf}
% \caption{Gluon PDF at the scale of $Q^2=1.9 \  \GeV^2$ for various values of the length-prior 
% weight $\alpha$ \cite{Chebyshev} using the Chebyshev parametrisation expanded to the 15th order.}
  \caption{The gluon density is shown at the starting scale. The black lines correspond to the error band of the gluon distribution using a standard parameterisation and it is to be compared to the case of the Chebyshev parameterisation \cite{Chebyshev}.}
 \label{fig:cheb}
\end{figure}

%
\item \bf{External PDFs:} \rm 
 \fitter provides the possibility to access external PDF sets, which can be used to compute 
theoretical predictions for the various processes of interest as implemented in \fitter. 
This is possible via an interface to \lhapdf~\cite{lhapdf,lhapdfweb} providing access to the 
global PDF sets available at different orders.
\fitter also allows to evolve PDFs from \lhapdf using the corresponding grids as an initial evolution boundary condition.
%locally through the \fitter or taken as provided by the \lhapdf grids. 
Figure \ref{fig:pdfs} illustrates the comparison of the PDFs accessed from \lhapdf as produced with the drawing 
tools available in \fitter.
\end{description}
%
\begin{figure}[!ht]
   \centering
   \includegraphics[width=8cm]{pdfs.pdf}
   \caption{Gluon density as extracted by various PDF groups at the scale of $Q^2=2 \ \GeV^2$, plotted using the drawing tools from \fitter.} 
 \label{fig:pdfs}
\end{figure}
%
\subsection{Representation of $\chi^2$}
\label{sec:chi2representation}

The PDF parameters are determined in \fitter by minimisation of the
$\chi^2$ function taking into account correlated and uncorrelated measurement uncertainties.
%The PDF parameters are extracted in a $\chi^2$ minimisation process.
%The construction of the $\chi^2$  accounts for the experimental uncertainties.
There are various forms of $\chi^2$ differing by method used to include 
the experimental uncertainties,
%There are various forms that can be used to represent the experimental uncertainties, 
e.g. using covariance matrix or providing nuisance parameters to encode dependence of 
each systematic source for each measurement data point, different scaling options, etc. 
%In addition, there are various methods to deal with correlated systematic (or statistical) uncertainties. 
The options available in \fitter are following.

\begin{description}
\item \bf {Covariance Matrix Representation:} \rm
For a data point $\mu_i$ with a corresponding theory prediction $m_i$, 
the $\chi^2$ function 
can be expressed in the following form:
%
\begin{eqnarray}
\chi^2 (m)& = & \sum_{i,k}(m_i-\mu_i)C^{-1}_{ik}(m_k-\mu_k),
\end{eqnarray}
were the experimental uncertainties are given in a form of covariance matrix $C_{i,k}$ for measurements in bins $i$ an $k$.
%The $\chi^2$ function depends on the theory parameters $m^i$ 
%(denoted as the vector $\boldsymbol{m}$).
The covariance matrix $C_{ik}$ is given by the sum of statistical, uncorrelated and correlated systematic contributions: 
\begin{eqnarray}
C_{ik}& = & C^{stat}_{ik}+C^{uncor}_{ik}+C^{sys}_{ik}.
\end{eqnarray}
With this representation the particular effect of a certain systematic source of the uncertainty 
cannot be distinguished from others.

\item \bf{Nuisance Parameters Representation:} \rm
For the case when systematic uncertainties are separated by sources
the $\chi^2$ form is expressed as
\label{sec:nuisance_representation}

%\begin{eqnarray} 
%    \chi^2\left(\boldsymbol{m},\boldsymbol{b}\right) &= &  
% \sum_i \frac{\left[m^i - \sum_j \gamma^i_j m^i b_j  - {\mu^i} \right]^2}
%{ \textstyle \delta^2_{i,{\rm stat}}\mu^i \left(m^i -  \sum_j \gamma^i_j m^i b_j\right)
%  + \left(\delta_{i,{\rm uncor}}\,  m^i\right)^2} \nonumber \\
%  &+& \sum_j b^2_j.
%\label{eq:aven}
%\end{eqnarray}
%{ \small
%\begin{equation} 
%    \chi^2\left(\boldsymbol{m},\boldsymbol{b}\right) =   
% \sum_i \frac{\left[m^i - \sum_j \gamma^i_j m^i b_j  - {\mu^i} \right]^2}
%{ \textstyle \delta^2_{i,{\rm stat}}\mu^i \left(m^i -  \sum_j \gamma^i_j m^i b_j\right)
%  + \left(\delta_{i,{\rm uncor}}\,  m^i\right)^2}+ \sum_j b^2_j.
%\label{eq:aven}
%\end{equation}}
{ \small
\begin{equation} 
    \chi^2\left(\boldsymbol{m},\boldsymbol{b}\right) =   
 \sum_i \frac{\left[ {\mu_i} - m_i \left( 1 - \sum_j \gamma^i_j b_j \right) \right]^2}
{ \textstyle \delta^2_{i,{\rm unc}}m_i^2 + \delta^2_{i,{\rm stat}}\, {\mu_i} m_i \left(1 - \sum_j \gamma^i_j b_j\right) }
  + \sum_j b^2_j,
\label{eq:aven}
\end{equation}}
%
were, ${\mu_i}$ is the central value of the measurement $i$ 
with its relative statistical $\delta_{i,\rm stat}$ 
and relative uncorrelated systematic uncertainty $\delta_{i,\rm unc}$.
Further, $\gamma^i_j$ quantifies the sensitivity of the
measurement to the correlated systematic source $j$. 
The function $\chi^2$ depends in addition on
 the set of systematic nuisance parameters $b_j$.
This definition of the $\chi^2$ function assumes that
systematic uncertainties are proportional to the central prediction values
(multiplicative errors), whereas the statistical uncertainties scale 
with the square root of the expected number of events. 

During the $\chi^2$ minimisation, the nuisance parameters $b_j$ and the PDFs are determined. 
%The nuisance parameters $b_j$ as well as the PDF parameters are free parameters of the fit. 
%The fit determines the best PDF parameters to
%the data taking into account correlated systematic shifts of the data. 
\item  \bf{Mixed Form Representation:} \rm
In some cases, the statistical and systematic uncertainties are provided in different forms.    
%It can happen that various parts of the systematic and statistical uncertainties are stored in different forms. 
For example, the correlated experimental systematic uncertainties are available as nuisance parameters
but the bin-to-bin statistical correlations are given in a form of covariance matrix.
%A situation can be envisaged when the correlated systematic experimental uncertainties are provided as nuisance parameters, but the statistical bin-to-bin correlations are given in the form of a covariance matrix. 
\fitter\ offers possibilities to include also the mixed 
%information, when provided, as well as any other mixed 
form of treating statistical, uncorrelated and correlated systematic uncertainties. 
%In the case of off-diagonal statistical uncertainties, the $\chi^2$ function
%\begin{equation} 
%\begin{eqnarray} 
% \label{eq:chi2gen}
%    \chi^2(\boldsymbol{m},\boldsymbol{b})& = & \sum_{ij} 
%         \left ( m^i - \sum_l \gamma^i_l(m^i)b_l - \mu^i \right)  C^{-1}_{{\rm stat.}~ij}(m^i,m^j) \nonumber \\  
%    && \left(  m^j - \sum_l \gamma^j_l(m^j)b_l - \mu^j \right) +  \sum_l b^2_l.
%\end{eqnarray}
%\end{equation}
%Here the scaling properties of the correlated systematic uncertainties 
%$\Gamma^i_j$ and
%of the covariance matrix $C_{{\rm stat.}~ij}$ are expressed as a dependence
%on $m_i$ and the dependence of $\delta_{\rm stat}$ on $b_j$ is ignored.
\end{description}


\subsection{Treatment of the Experimental Uncertainties}
\label{sec:experimentalerrors}


%Three distinct cases are implemented in \fitter\ and reviewed here:
Three distinct methods for propagating experimental uncertainties to PDFs are implemented in \fitter and reviewed here:
%\fitter\ provides three methods for assessing the experimental uncertainties on PDFs: 
the Hessian, Offset, and Monte Carlo method.
%Figure \ref{fig:error} illustrates the difference between the Hessian and Monte-Carlo methods both of which can be applied and plotted with \fitter.
%\begin{figure}[!ht]
%   \centering
%   \includegraphics[width=8cm]{error.pdf}
%   \put (-206, 68) {Hessian}
%   \put (-86, 68) {Monte Carlo}
%   \caption{Differences in the experimental uncertainties on the gluon (left) and d-valence quark (right) densities extracted 
%       through different methods in \fitter: Hessian(left) versus Monte Carlo (right).} 
% \label{fig:error}
%\end{figure}
\begin{description}
\item \bf{Hessian method:} \rm
The PDF uncertainties reflecting the uncertainties in experimental data are esitimated by 
examining the shape of $\chi^2$ in the neighborhood of the minimum \cite{Pumplin:2001ct}.
%The technique developed in~\cite{Pumplin:2001ct} presents an estimate of PDF uncertainties 
%reflecting the experimental precision of data used in the QCD fit by examining the behavior 
%of $\chi^2$ in the neighborhood of the minimum. 
Following approach of \cite{Pumplin:2001ct}, the Hessian matrix is defined by the second 
derivatives of $\chi^2$ on the fitted PDF parameters. The matrix is diagonalized and the 
Hessian eigenvectors are computed. 
Due to orthogonality, these vectors correspond to statistically independent sources of the
uncertainties in the PDFs obtained.
%
%This is known as the Hessian or error matrix method. 
%The Hessian matrix is built by the second derivatives of $\chi^2$ at the minimum. 
%The Hessian matrix is diagonalised through an iterative procedure and its PDF eigenvectors
%are obtained, which correspond to the orthogonal sources of uncertainties on the obtained PDF.

\item \bf{Offset  method:} \rm
The Offset method \cite{Botje:2001fx} uses
%Another method to propagate the correlated systematic experimental uncertainties from 
%the measurements to PDFs \cite{Botje:2001fx} is Offset method.
%, which has the practical advantage that it does not require the inversion of a large measurement covariance matrix.
%
also the $\chi^2$ function for the central fit for which only
uncorrelated uncertainties are taken into account. 
The goodness of the fit can no longer be judged from the $\chi^2$ since correlated uncertainties are ignored. 
The correlated uncertainties are propagated into the PDF uncertainties performing the variants 
of fit with the experimental data varied by $\pm 1 \sigma$ from the central value  
for each systematic source.
Since the resulting deviation of the PDF parameters from the ones obtained in the central 
fit are statistically independent, they are combined in quadrature to arive to the total 
PDF systematic uncertainty.
%
%Instead, the correlated systematic uncertainties of the measurement are used to estimate 
%the errors on the PDF parameters as follows.
%The cross section is varied by $\pm 1 \sigma$ from the central value 
%for each systematic source and the fit is performed. 
%After this has been done for all sources, the 
%resulting deviations of each of these fits from the central PDF parameters are added in quadrature. 

In most cases, the uncertainties estimated by the offset method are larger than 
those from the Hessian method.
%as the offset method does not use the information on correlated systematic uncertainties 
%in the central fit.

\item \bf{Monte Carlo method:} \rm
The Monte-Carlo technique \cite{Giele:1998gw, mcmethod2} can be used to determine PDF uncertainties.
The uncertainties are estimated using the pseudo-data replicas (typically $>100$) 
randomly generated from the measurement central values and their systematic and statistical uncertainties 
taking into account all point-to-point correlations.
%
The QCD fit is performed for each replica and the PDF central values with their 
experimental uncertainties are estimated using distribution of the PDF parameters over these fits, i.e.
the mean values and standard deviations over the replicas.
%
%The preparation of the data is repeated for large $N$ ($>100$ times) and for each of these replicas a QCD fit is performed. 
%The PDF central values and experimental uncertainties are estimated using the mean values 
%and standard deviations over the replicas.

%The MC method represents one of the main concepts of the NNPDF group.
The MC method was checked against the standard error estimation of the PDF uncertainties obtained by the Hessian method. 
A good agreement was found between the methods 
once the Gaussian distribution of statistic and systematic uncertainties is assumed in the MC 
approach~\cite{hera-lhc:report2009}.
%when employing for the MC approach the assumption that uncertainties 
%(statistical and systematic) follow Gaussian distribution~\cite{hera-lhc:report2009}. 
This comparison is illustrated in Fig.~\ref{fig:mchessian}. 
Similar findings were reported by the MSTW global analysis~\cite{Watt:2012tq}. 
\begin{figure}[!ht]
 \centering
  \includegraphics[width=7cm,height=7cm]{mchessian.pdf}
  \caption{Comparison between the standard error calculations as employed by the Hessian approach (black lines) 
           and the MC approach (with more than 100 replicas) assuming Gaussian distribution for uncertainty 
           distributions, shown here for each replica 
          (green lines) together with the evaluated standard deviation (red lines) \cite{hera-lhc:report2009}.
          The black lines in the figure are mostly covered by the red lines.}
  \label{fig:mchessian}        
\end{figure}
\end{description}
%
The nuisance parameter representation of $\chi^2$ in Eq.~\ref{eq:aven} is derived assuming 
symmetric experimental errors, however, the published systematic uncertainties are 
rather often asymmetric.
%Generally, the experimental uncertainties using nuisance parameters are 
%symmetrised when QCD fits 
%are performed, however often the provided uncertainties are rather asymmetric.
\fitter provides the possibility to use asymmetric systematic uncertainties.
The implementation relies on the assumption that 
asymmetric uncertainties can be described by a parabolic function
and the nuisance parameter in Eq.~\ref{eq:aven} is modified as follows
\begin{equation}
  m_i(1-\sum_j \gamma^i_{j} b_j) \to 
m_i\left(1-\sum_j b_j(\omega^i_{j}b_j + \gamma^i_{j})\right),
\end{equation}
where the coefficients $\omega^i_{j}$, $\gamma^i_{j}$ are defined  
by the up and down values of the systematic uncertainties,
%up and down shifts of the cross sections to a nuisance parameter, 
$S_{ij}^{\pm}$,
\begin{equation}
  \omega^i_{j}=\frac{1}{2}\left(S_{ij}^{+}+S_{ij}^{-}\right), \\
  \gamma^i_{j}=\frac{1}{2}\left(S_{ij}^{+}-S_{ij}^{-}\right). 
\end{equation}
%For this case the definition of the $\chi^2$ from Eq.~\ref{eq:aven} is extended with the parabolic approximation 
%for asymmetric uncertainties, such that the expected cross section is adjusted to be
%\begin{equation}
%  m_i(1-\sum_j \gamma^i_{j} b_j) \to 
%m_i\left(1-\sum_j b_j(\omega^i_{j}b_j + \gamma^i_{j})\right).
%\end{equation}
The minimisation is performed using fixed number of iterations (typically ten), with rapid convergence.





\subsection{Treatment of the Theoretical Input Parameters}

The results of a QCD fit depend not only on the input data but also on the 
input parameters used in the theoretical calculations. Nowadays, recent PDF sets 
address the impact of the choices of theoretical parameters by providing
alternative PDFs with different choices of the mass of the charm quarks $m_c$, 
mass of the bottom quarks $m_b$ and the value of $\asmz$, etc. 
Another important issue is the choice of the functional form for the PDFs at the 
starting scale and the value of the starting scale itself. \fitter provides 
possibility of different user choices of various input parameters of the theory.
%a platform in which such choices can readily be varied within a common framework.

\subsection{Bayesian Reweighting Techniques}
As alternative to performing a full QCD fit, \fitter allows to assess the impact of including new
data in an existing fit using the Bayesian Reweighting technique. Since no fit is performed, the method
provides a fast estimate of the impact of new data on PDFs. 
Bayesian reweighting was first proposed, for the PDF sets delivered in form of Monte Carlo replicas ensembles, in~\cite{Giele:1998gw} and further developed by the NNPDF Collaboration~\cite{Ball:2011gg,Ball:2010gb}. 
More recently, a method to preform Bayesian Reweighting studies starting from PDF fits where uncertainties
are provided in form of parameter eigenvectors has been also developed~\cite{Watt:2012tq}. The latter is 
based on generating replica set by introducing Gaussian fluctuations on the central PDF set with a variance
determined by the PDF uncertainty given by the eigenvectors.

As an alternative to a complete QCD fit, the reweighting method (Bayesian Reweighting) is available in \fitter.
The method provides a fast estimate of the impact of new data on PDFs. 
The original suggestion~\cite{Giele:1998gw} was developed by the NNPDF 
collaboration \cite{Ball:2011gg,Ball:2010gb} and later extended~\cite{Watt:2012tq}
to work not only on the NNPDF replicas, but also on the eigenvectors provided by most PDF groups. 

Within the Bayesian Reweighting technique the PDF probability distributions are modified with weights 
to account for the difference between theory predictions and new data.
In the NNPDF method the PDFs are constructed as ensembles of $N_{\rm rep}$ parton 
distribution functions and observables $\mathcal{O}(\mathrm{PDF})$ are conventionally calculated from the average
of the predictions obtained from the ensemble: 
\begin{equation}
\langle\mathcal{O}(\mathrm{PDF})\rangle =  \frac{1}{N_{\mathrm{rep}}} \sum_{k=1}^{N_{\mathrm{rep}}} \mathcal{O}(\mathrm{PDF}_k).
\end{equation}
In the case of PDF uncertainties provided by standard Hessian eigenvector error sets, this can be achieved 
by creating the $k$-th random replica by introducing random fluctuations around the central PDF set.

\begin{equation}
 w_k = \frac{(\chi^2_k)^{\frac{1}{2} (N_{\mathrm{data}}-1) } e^{-\frac{1}{2}\chi^2_k}}{ \frac{1}{N_{\mathrm{rep}}} \sum^{N_{\mathrm{rep}}}_{k=1}(\chi^2_k)^{\frac{1}{2}(N_{\mathrm{data}}-1)} e^{-\frac{1}{2}\chi^2_k}  },
\end{equation}

where $N_{\mathrm{data}}$ is the number of new data points, $k$ denotes the specific replica for which the weight is calculated and $\chi^2_k$ is the chi-square of the new data obtained using the $k$-th PDF replica:

{\small
\begin{equation}
 \chi^2 (y,\mathrm{PDF}_k) = \sum_{i,j=1}^{N_{\mathrm{data}}} (y_i - y_i(\mathrm{PDF}_k)) \sigma^{-1}_{ij} (y_j-y_j(\mathrm{PDF}_k))\,. 
\end{equation}
}

%The new, reweighted PDFs commonly are chosen to be based upon a smaller number of PDF sets compared to the input because replicas 
%that are incompatible with the data are discarded in order to create a more stream-lined PDF set.
%
From all the resulting PDF replicas, those providing predictions incompatible with the measurements are discarded. 
Therefore, reweighted PDFs encompass less replicas than used in the input.

The number of effective replicas of a reweighted sets, that is the size of an equiprobable replicas set 
containing the same amount of information as the reweighted set in question, is measured by the Shannon
Entropy

\begin{equation}
\label{eq:shannon}
N_\mathrm{eff}\equiv 
\exp\left\{\frac{1}{N_\mathrm{rep}}\sum_{k=1}^N\mathrm{rep}w_k\ln(N_\mathrm{rep}/w_k)\right\}\,.
\end{equation}

On the one hand there is no reason in generating a final unweighted set that has a number of replicas
(significantly) larger than $N_\mathrm{eff}$ as no extra information is gained. On the other hand it is
advisable to start from a prior PDF set which has as many replicas as possible in order to have a more
accurate posterior set at the end of the reweighting procedure.






